{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from operator import itemgetter    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data from Pre-Processing\n",
    "* Missing values HAVE been imputed.\n",
    "* No PCA performed yet, no 1hot encoding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# Import Data from PreProcessing\n",
    "#\n",
    "#####################\n",
    "\n",
    "df= pd.read_csv('../_data/operations_imputed_CLEAN_v2.csv', index_col=0)\n",
    "\n",
    "df.drop(['race'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the X and y DataFrames for REGRESSION Model\n",
    "\n",
    "\n",
    "  * create y\n",
    "  * create X (complete with all the features)\n",
    "  * drop the features we identified as not meeting impact threshold.\n",
    "  *  * Target = `LOS` (continuous variable)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 76742 entries, 8 to 128030\n",
      "Data columns (total 27 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   category_id      76742 non-null  object \n",
      " 1   age              76742 non-null  int64  \n",
      " 2   sex              76742 non-null  object \n",
      " 3   weight           76742 non-null  float64\n",
      " 4   height           76742 non-null  float64\n",
      " 5   hr               76742 non-null  float64\n",
      " 6   pip              76742 non-null  float64\n",
      " 7   pmean            76742 non-null  float64\n",
      " 8   rr               76742 non-null  float64\n",
      " 9   spo2             76742 non-null  float64\n",
      " 10  vt               76742 non-null  float64\n",
      " 11  chloride         76742 non-null  float64\n",
      " 12  creatinine       76742 non-null  float64\n",
      " 13  glucose          76742 non-null  float64\n",
      " 14  hb               76742 non-null  float64\n",
      " 15  hco3             76742 non-null  float64\n",
      " 16  lymphocyte       76742 non-null  float64\n",
      " 17  platelet         76742 non-null  float64\n",
      " 18  potassium        76742 non-null  float64\n",
      " 19  sodium           76742 non-null  float64\n",
      " 20  total_bilirubin  76742 non-null  float64\n",
      " 21  wbc              76742 non-null  float64\n",
      " 22  icu_visit        76742 non-null  int64  \n",
      " 23  or_duration      76742 non-null  float64\n",
      " 24  anesth_duration  76742 non-null  float64\n",
      " 25  department       76742 non-null  object \n",
      " 26  antype           76742 non-null  object \n",
      "dtypes: float64(21), int64(2), object(4)\n",
      "memory usage: 16.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# When doing a Categorical Model, reinsert 'prolonged_LOS' and instead, drop 'LOS'\n",
    "\n",
    "## Features to retain are those in X that will be used in training. Exludued features are features such as Operation_ID, Subject_ID..\n",
    "features_to_retain = ['category_id','age','sex',\t'weight',\t'height',\t'hr',\t'pip',\t'pmean',\t'rr',\t'spo2',\t'vt',\t'chloride',\t'creatinine',\t'glucose',\t'hb',\t'hco3',\t'lymphocyte',\t'platelet',\t'potassium',\t'sodium',\t'total_bilirubin',\t'wbc',\t'icu_visit',\t'or_duration',\t'anesth_duration',\t'department','antype'] \n",
    "\n",
    "## Create the Y, the Target\n",
    "y = df['LOS']\n",
    "\n",
    "## Create X the Features for Train/Test/Validate\n",
    "X = df.drop('LOS', axis=1)\n",
    "X= X[features_to_retain]\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Define the columns that you want to scale and cast to strings\n",
    "numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numeric_features = [col for col in numeric_features if col not in ['a', 'b', 'c']]\n",
    "string_features = ['category_id','antype','sex','department', 'icu_visit'] \n",
    "\n",
    "# Define transformers\n",
    "numeric_transformer = StandardScaler()\n",
    "string_transformer = FunctionTransformer(lambda x: x.astype(str), validate=False)\n",
    "\n",
    "# Combine transformers into a preprocessor with ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('str', string_transformer, string_features)\n",
    "    ])\n",
    "\n",
    "# Create a full pipeline by combining with an estimator, for example, a classifier\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', MODEL_NAME )],\n",
    "    remainder='passthrough')  # Leaves the rest of the columns alone\n",
    "\n",
    "\n",
    "\n",
    "# Split the data (example using a generic train_test_split function)\n",
    "# X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "# Fit the model with the training data\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# Now you can use the model to make predictions or evaluate\n",
    "# y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training-Test-Validation Split\n",
    "\n",
    "- Training Set (80% of total): \n",
    "  - Used to train the models.\n",
    "- Validation Set (20% of Traning Set ): \n",
    "  - Used to fine-tune hyperparameters, select models, and monitor training progress.  \n",
    "- Testing Set (20% of total): \n",
    "  - Used to evaluate the final model's performance on unseen data and estimate its generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (46044, 27)\n",
      "X_validate shape: (15349, 27)\n",
      "X_test shape: (15349, 27)\n",
      "y_train shape: (46044,)\n",
      "y_validate shape: (15349,)\n",
      "y_test shape: (15349,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SPLIT = .2\n",
    "TRAINING_SPLIT = 1-TEST_SPLIT\n",
    "VALIDATION_SPLIT = .2\n",
    "\n",
    "\n",
    "def split_data(X, y, test_split=0.2, validation_split=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Splits data into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Features data.\n",
    "    - y: Target variable.\n",
    "    - test_split: Fraction of the data to be used as test set.\n",
    "    - validation_split: Fraction of the training data to be used as validation set.\n",
    "    - random_state: Seed for the random number generator.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing the split data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_split, random_state=random_state)\n",
    "\n",
    "    # Adjust validation split to account for the initial test split\n",
    "    validation_size = validation_split / (1 - test_split)\n",
    "\n",
    "    # Split the training data again into training and validation sets\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(\n",
    "        X_train, y_train, test_size=validation_size, random_state=random_state)\n",
    "\n",
    "    # Create a dictionary to hold the data splits\n",
    "    data_splits = {\n",
    "        'X_train': X_train,\n",
    "        'X_validate': X_validate,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_validate': y_validate,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "\n",
    "    return data_splits\n",
    "\n",
    "\n",
    "'''# Example usage:\n",
    "# Assuming X and y are your data and labels\n",
    "data_splits = split_data(X, y, test_split=TEST_SPLIT, validation_split=VALIDATION_SPLIT, random_state=85100)\n",
    "\n",
    "# Print the shapes of the splits\n",
    "for key, value in data_splits.items():\n",
    "    shape = value.shape\n",
    "    print(f\"{key} shape: {shape}\")'''\n",
    "\n",
    "## Call data split function:\n",
    "data_splits = split_data(X, y, test_split=TEST_SPLIT, validation_split=VALIDATION_SPLIT, random_state=85100)\n",
    "for key, value in data_splits.items():\n",
    "    shape = value.shape\n",
    "    print(f\"{key} shape: {shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize and/or Normalize X\n",
    "* We do NOT scale Y, the target\n",
    "* We fit the StandardScaler on X_training and then transform both your training and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class to modify Features: to be either cast as Str or Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawFeats:\n",
    "    def __init__(self, feats):\n",
    "        self.feats = feats\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.feats]\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def cast_columns_as_string(self, X, columns_to_cast):\n",
    "        X[columns_to_cast] = X[columns_to_cast].astype(str)\n",
    "        return X\n",
    "    \n",
    "    def get_numeric_columns(self, X):\n",
    "        numeric_columns = X.select_dtypes(include=['int', 'float']).columns\n",
    "        return numeric_columns\n",
    "    \n",
    "COLS_TO_CAST = ['category_id','antype','sex','department', 'icu_visit'] \n",
    "raw_feats = RawFeats(feats=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### \n",
    "## SCALE X_train and X_validate\n",
    "#########\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Two preprocessors, one for scale and one for casting\n",
    "\n",
    "# Get numerical columns for scaling\n",
    "# COLS_TO_SCALE = RawFeats.get_numeric_columns(features_to_retain)\n",
    "\n",
    "preprocessor_nums = Pipeline(steps=[(\"rawFeats\", raw_feats,'scaler', StandardScaler())])\n",
    "# preprocessor_cats = Pipeline(steps=[('caster', RawFeats.cast_columns_as_string(X_train,COLS_TO_CAST))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the preprocessor with separate transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', preprocessor_nums, X_train),\n",
    "        # ('str', preprocessor_cats, )\n",
    "    ],\n",
    "    remainder='passthrough'  # Leaves the rest of the columns alone\n",
    ")\n",
    "\n",
    "# # Fit on the training data\n",
    "# preprocessor.fit(X_train)\n",
    "\n",
    "# # Transform the training and validation data\n",
    "# X_train_scaled = preprocessor.transform(X_train)\n",
    "# X_validate_scaled = preprocessor.transform(X_validate)\n",
    "\n",
    "# Now X_train_scaled and X_validate_scaled have the specified columns scaled, and the rest are unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "#  SIMPLE LINEAR REGRESSION Pipeline -\n",
    "#  -- No tuning. \n",
    "########################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "categorical_transform = Pipeline([('one-hot-encode', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))])\n",
    "\n",
    "preprocessing_df = ColumnTransformer([('categorical', categorical_transform, COLS_TO_CAST)])\n",
    "\n",
    "pipeline_base = Pipeline([('preprocessing', preprocessing_df),\n",
    "                          ('model', LinearRegression())])\n",
    "pipeline_base.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lin = pipeline_base.predict(X_validate)\n",
    "r2 = r2_score(y_validate, y_pred_lin)\n",
    "rmse = mean_squared_error(y_validate, y_pred_lin, squared=False)\n",
    "print(f'R-squared of base model: {r2}')\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Models - Baselines  \n",
    "\n",
    "Models employed:\n",
    "* ExtraTreesRegressor\n",
    "* Random Forest\n",
    "* XGBRegressor\n",
    "* CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "#  Ensemble Pipeline -\n",
    "#  -- No tuning. \n",
    "########################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "model_list= [LinearRegression(),ExtraTreesRegressor (n_jobs=-1),RandomForestRegressor(n_jobs=-1),XGBRegressor(n_jobs=-1)]\n",
    "model_names = [\"Linear Regression\",\"ExtraTreesRegressor\", \"Random Forest\",\"XGBRegressor\"]\n",
    "\n",
    "\n",
    "ModScores = {}\n",
    "\n",
    "categorical_transform = Pipeline([('one-hot-encode', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))])\n",
    "\n",
    "preprocessing_df = ColumnTransformer([('categorical', categorical_transform, COLS_TO_CAST)])\n",
    "\n",
    "for model_names, model in zip(model_names, model_list):\n",
    "    pipeline_base = Pipeline([('preprocessing', preprocessing_df),\n",
    "                          ('model', model)])\n",
    "    pipeline_base.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipeline_base.predict(X_validate)\n",
    "    \n",
    "    # Calculate the R-squared value\n",
    "    r2 = r2_score(y_validate, y_pred)\n",
    "    rmse = mean_squared_error(y_validate, y_pred, squared=False)\n",
    "    \n",
    "    ModScores[model_names] = rmse\n",
    "    \n",
    "    print(f\"{model}: R2: {r2:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "print(\"_\"*100)\n",
    "for key, value in sorted(ModScores.items(), key=itemgetter(1), reverse=False):\n",
    "    print(f\"{key}: RMSE: {value:.3f}\")\n",
    "\n",
    "########################\n",
    "## Output:\n",
    "########################\n",
    "'''Random Forest: RMSE: 3.995\n",
    "XGBRegressor: RMSE: 4.004\n",
    "ExtraTreesRegressor: RMSE: 4.032\n",
    "Linear Regression: RMSE: 9515250466.184'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "#  CatBoost - Baseline\n",
    "# \n",
    "########################\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import Pool\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "###############\n",
    "# Specify categorical feature indices\n",
    "categorical_features_indices = [0, 2, 22,25, 26]\n",
    "\n",
    "\n",
    "# Create the Pool for training data\n",
    "train_pool = Pool(data=X_train, label=y_train, cat_features=categorical_features_indices)\n",
    "\n",
    "# If you have a validation dataset\n",
    "validation_pool = Pool(data=X_validate, label=y_validate, cat_features=categorical_features_indices)\n",
    "\n",
    "\n",
    "# Instantiate CatBoostRegressor with the best hyperparameters\n",
    "cat_model = CatBoostRegressor()\n",
    "\n",
    "cat_model.fit(\n",
    "    train_pool,\n",
    "    eval_set=validation_pool,  # Remove this if you don't have a validation set\n",
    "    verbose=10,  # This will print the progress every 10 iterations\n",
    "    plot=True    # This will plot the learning curve (only works in Jupyter notebooks)\n",
    ")\n",
    "\n",
    "predictions = cat_model.predict(X_validate)  # If you used Pool, the data here should not be the Pool object but raw data.\n",
    "\n",
    "# Calculate the R-squared value\n",
    "r2 = r2_score(y_validate, predictions)\n",
    "rmse = mean_squared_error(y_validate, predictions, squared=False)\n",
    "\n",
    "print(f'R-squared of base model: {r2}')\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "\n",
    "########################\n",
    "## Output:\n",
    "########################\n",
    "'''Shrink model to first 619 iterations.\n",
    "R-squared of base model: 0.5159662312506688\n",
    "RMSE of the base model: 3.884'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning -  Optimize Model with Hyperparameter Tuning via Grid Search\n",
    "\n",
    "## CatBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "#  STANDALONE TUNING  - CatBoost\n",
    "# \n",
    "########################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from catboost import Pool, CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Scorer for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "#  RMSE Scorer for GridSearch\n",
    "# \n",
    "########################\n",
    "\n",
    "def rmse_scorer(estimator, X, y):\n",
    "    \"\"\"\n",
    "    Custom scoring function to calculate the negative RMSE (Root Mean Squared Error).\n",
    "\n",
    "    Parameters:\n",
    "        estimator: Scikit-learn estimator object\n",
    "            The model to be evaluated.\n",
    "        X: array-like or pd.DataFrame\n",
    "            Features for prediction.\n",
    "        y: array-like or pd.Series\n",
    "            True target values.\n",
    "\n",
    "    Returns:\n",
    "        float\n",
    "            Negative RMSE value.\n",
    "    \"\"\"\n",
    "    y_pred = estimator.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    rmse = sqrt(mse)\n",
    "    \n",
    "    # Return negative RMSE for grid search to minimize\n",
    "    return -rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost -  Perform GridSearch (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "categorical_features_indices =[0, 2, 22,25, 26]\n",
    "\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'iterations': [100, 200, 300],      # Number of boosting iterations\n",
    "    'depth': [6, 8, 10],                # Depth of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate  \n",
    "    }\n",
    "\n",
    "# Create a CatBoostRegressor model\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search_lin = GridSearchCV(estimator=catboost_model, param_grid=param_grid, cv=5, scoring=rmse_scorer, n_jobs=-1, error_score='raise')\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search_lin.fit(X_train, y_train, cat_features=categorical_features_indices)\n",
    "\n",
    "# Print the best hyperparameters and corresponding MSE score\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(grid_search_lin.best_params_)\n",
    "print(\"Best RMSE score:\", -grid_search_lin.best_score_)\n",
    "\n",
    "# Get the best trained model\n",
    "best_catboost_model_lin = grid_search_lin.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "validation_predictions = best_catboost_model_lin.predict(X_validate)\n",
    "\n",
    "########################\n",
    "## Output:\n",
    "########################\n",
    "'''Best hyperparameters found:\n",
    "{'depth': 10, 'iterations': 300, 'learning_rate': 0.1}\n",
    "Best RMSE score: 3.834154628241867'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoostRegressor - Train Model on Optimized Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "#  OPTIMIZED - CatBoost\n",
    "# \n",
    "########################\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import Pool\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "###############\n",
    "# Specify categorical feature indices\n",
    "categorical_features_indices = [0, 2, 22,25, 26]\n",
    "\n",
    "\n",
    "# Create the Pool for training data\n",
    "train_pool = Pool(data=X_train, label=y_train, cat_features=categorical_features_indices)\n",
    "\n",
    "# If you have a validation dataset\n",
    "validation_pool = Pool(data=X_validate, label=y_validate, cat_features=categorical_features_indices)\n",
    "\n",
    "#########\n",
    "# Best hyperparameters found (after hyperparameter tuning)\n",
    "best_params = {'depth': 8, 'iterations': 200, 'learning_rate': 0.1}\n",
    "\n",
    "# Instantiate CatBoostRegressor with the best hyperparameters\n",
    "cat_model_lin = CatBoostRegressor(**best_params)\n",
    "\n",
    "cat_model_lin.fit(\n",
    "    train_pool,\n",
    "    eval_set=validation_pool,  # Remove this if you don't have a validation set\n",
    "    verbose=10,  # This will print the progress every 10 iterations\n",
    "    plot=True    # This will plot the learning curve (only works in Jupyter notebooks)\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_cat_lin_r2 = cat_model_lin.predict(X_validate)  # If you used Pool, the data here should not be the Pool object but raw data.\n",
    "\n",
    "\n",
    "# Calculate the R-squared value\n",
    "r2 = r2_score(y_validate, y_pred_cat_lin_r2)\n",
    "rmse = mean_squared_error(y_validate, y_pred_cat_lin_r2, squared=False)\n",
    "\n",
    "\n",
    "print(f'R-squared of base model: {r2}')\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "\n",
    "\n",
    "########################\n",
    "## Output:\n",
    "########################\n",
    "'''bestTest = 3.863708828\n",
    "bestIteration = 177\n",
    "\n",
    "Shrink model to first 178 iterations.\n",
    "R-squared of base model: 0.5210853587572815\n",
    "RMSE of the base model: 3.864'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_validate - y_pred_cat_lin_r2\n",
    "\n",
    "# Create a scatter plot of residuals vs. predicted values\n",
    "plt.scatter(y_pred_cat_lin_r2, residuals)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_validate,y_pred_cat_lin_r2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values')\n",
    "plt.plot([min(y_validate), max(y_validate)], [min(y_validate), max(y_validate)], color='red', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(residuals, bins=30)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../_output/catboost_model_regression_opt.pickle', 'wb') as f:\n",
    "    pickle.dump(cat_model_lin, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lighthouse_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
