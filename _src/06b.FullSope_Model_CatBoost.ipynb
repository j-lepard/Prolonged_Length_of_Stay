{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from operator import itemgetter    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data from Pre-Processing\n",
    "* In this scenario, we are using only the surgeries with the greatest volume count (id_category: 08R). \n",
    "* Missing values HAVE been imputed.\n",
    "* No PCA performed yet, no 1hot encoding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# Trial with SINGLE Operation Category\n",
    "#\n",
    "#####################\n",
    "\n",
    "df= pd.read_csv('../_data/operations_imputed_CLEAN_v2.csv', index_col=0)\n",
    "\n",
    "# Going for it - doing the whole deal\n",
    "# df = df[df['category_id']=='08R']\n",
    "df.drop(['race'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 76742 entries, 8 to 128030\n",
      "Data columns (total 37 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   op_id            76742 non-null  int64  \n",
      " 1   subject_id       76742 non-null  int64  \n",
      " 2   hadm_id          76742 non-null  int64  \n",
      " 3   opdate           76742 non-null  int64  \n",
      " 4   age              76742 non-null  int64  \n",
      " 5   sex              76742 non-null  object \n",
      " 6   weight           76742 non-null  float64\n",
      " 7   height           76742 non-null  float64\n",
      " 8   asa              76742 non-null  float64\n",
      " 9   department       76742 non-null  object \n",
      " 10  antype           76742 non-null  object \n",
      " 11  icd10_pcs        76742 non-null  object \n",
      " 12  category_desc    76742 non-null  object \n",
      " 13  desc_short       76742 non-null  object \n",
      " 14  category_id      76742 non-null  object \n",
      " 15  hr               76742 non-null  float64\n",
      " 16  pip              76742 non-null  float64\n",
      " 17  pmean            76742 non-null  float64\n",
      " 18  rr               76742 non-null  float64\n",
      " 19  spo2             76742 non-null  float64\n",
      " 20  vt               76742 non-null  float64\n",
      " 21  chloride         76742 non-null  float64\n",
      " 22  creatinine       76742 non-null  float64\n",
      " 23  glucose          76742 non-null  float64\n",
      " 24  hb               76742 non-null  float64\n",
      " 25  hco3             76742 non-null  float64\n",
      " 26  lymphocyte       76742 non-null  float64\n",
      " 27  platelet         76742 non-null  float64\n",
      " 28  potassium        76742 non-null  float64\n",
      " 29  sodium           76742 non-null  float64\n",
      " 30  total_bilirubin  76742 non-null  float64\n",
      " 31  wbc              76742 non-null  float64\n",
      " 32  LOS              76742 non-null  float64\n",
      " 33  prolonged_LOS    76742 non-null  int64  \n",
      " 34  icu_visit        76742 non-null  int64  \n",
      " 35  or_duration      76742 non-null  float64\n",
      " 36  anesth_duration  76742 non-null  float64\n",
      "dtypes: float64(23), int64(7), object(7)\n",
      "memory usage: 22.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the X and y DataFrames\n",
    "\n",
    "  * create y\n",
    "  * create X (complete with all the features)\n",
    "  * drop the features we identified as not meeting impact threshold. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 76742 entries, 8 to 128030\n",
      "Data columns (total 27 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   category_id      76742 non-null  object \n",
      " 1   age              76742 non-null  int64  \n",
      " 2   sex              76742 non-null  object \n",
      " 3   weight           76742 non-null  float64\n",
      " 4   height           76742 non-null  float64\n",
      " 5   hr               76742 non-null  float64\n",
      " 6   pip              76742 non-null  float64\n",
      " 7   pmean            76742 non-null  float64\n",
      " 8   rr               76742 non-null  float64\n",
      " 9   spo2             76742 non-null  float64\n",
      " 10  vt               76742 non-null  float64\n",
      " 11  chloride         76742 non-null  float64\n",
      " 12  creatinine       76742 non-null  float64\n",
      " 13  glucose          76742 non-null  float64\n",
      " 14  hb               76742 non-null  float64\n",
      " 15  hco3             76742 non-null  float64\n",
      " 16  lymphocyte       76742 non-null  float64\n",
      " 17  platelet         76742 non-null  float64\n",
      " 18  potassium        76742 non-null  float64\n",
      " 19  sodium           76742 non-null  float64\n",
      " 20  total_bilirubin  76742 non-null  float64\n",
      " 21  wbc              76742 non-null  float64\n",
      " 22  icu_visit        76742 non-null  int64  \n",
      " 23  or_duration      76742 non-null  float64\n",
      " 24  anesth_duration  76742 non-null  float64\n",
      " 25  department       76742 non-null  object \n",
      " 26  antype           76742 non-null  object \n",
      "dtypes: float64(21), int64(2), object(4)\n",
      "memory usage: 16.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Label = LOS\n",
    "\n",
    "# When doing a Categorical Model, reinsert 'prolonged_LOS' and instead, drop 'LOS'\n",
    "\n",
    "features_to_retain = ['category_id','age','sex',\t'weight',\t'height',\t'hr',\t'pip',\t'pmean',\t'rr',\t'spo2',\t'vt',\t'chloride',\t'creatinine',\t'glucose',\t'hb',\t'hco3',\t'lymphocyte',\t'platelet',\t'potassium',\t'sodium',\t'total_bilirubin',\t'wbc',\t'icu_visit',\t'or_duration',\t'anesth_duration',\t'department','antype'] \n",
    "\n",
    "y = df['LOS']\n",
    "X = df.drop('LOS', axis=1)\n",
    "# Get a list of column names with data type 'object'\n",
    "\n",
    "\n",
    "X= X[features_to_retain]\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Cols to encode: ['category_id', 'antype', 'sex', 'department']\n",
      "Numerical Cols to scale: Index(['age', 'weight', 'height', 'hr', 'pip', 'pmean', 'rr', 'spo2', 'vt',\n",
      "       'chloride', 'creatinine', 'glucose', 'hb', 'hco3', 'lymphocyte',\n",
      "       'platelet', 'potassium', 'sodium', 'total_bilirubin', 'wbc',\n",
      "       'icu_visit', 'or_duration', 'anesth_duration'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 76742 entries, 8 to 128030\n",
      "Data columns (total 27 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   category_id      76742 non-null  object \n",
      " 1   age              76742 non-null  int64  \n",
      " 2   sex              76742 non-null  object \n",
      " 3   weight           76742 non-null  float64\n",
      " 4   height           76742 non-null  float64\n",
      " 5   hr               76742 non-null  float64\n",
      " 6   pip              76742 non-null  float64\n",
      " 7   pmean            76742 non-null  float64\n",
      " 8   rr               76742 non-null  float64\n",
      " 9   spo2             76742 non-null  float64\n",
      " 10  vt               76742 non-null  float64\n",
      " 11  chloride         76742 non-null  float64\n",
      " 12  creatinine       76742 non-null  float64\n",
      " 13  glucose          76742 non-null  float64\n",
      " 14  hb               76742 non-null  float64\n",
      " 15  hco3             76742 non-null  float64\n",
      " 16  lymphocyte       76742 non-null  float64\n",
      " 17  platelet         76742 non-null  float64\n",
      " 18  potassium        76742 non-null  float64\n",
      " 19  sodium           76742 non-null  float64\n",
      " 20  total_bilirubin  76742 non-null  float64\n",
      " 21  wbc              76742 non-null  float64\n",
      " 22  icu_visit        76742 non-null  int64  \n",
      " 23  or_duration      76742 non-null  float64\n",
      " 24  anesth_duration  76742 non-null  float64\n",
      " 25  department       76742 non-null  object \n",
      " 26  antype           76742 non-null  object \n",
      "dtypes: float64(21), int64(2), object(4)\n",
      "memory usage: 16.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Indentify the columns that need to be either cast as Str or Scaled\n",
    "############################################################\n",
    "\n",
    "COLS_TO_CAST = ['category_id','antype','sex','department'] #When restoring scope to full category list, add cat_id here.\n",
    "# Convert the object data type columns to string\n",
    "\n",
    "X[COLS_TO_CAST] = X[COLS_TO_CAST].astype(str)\n",
    "\n",
    "# Filter columns with dtype 'numeric' for scaling later in the Pipleine\n",
    "COLS_TO_SCALE = X.select_dtypes(include=['int', 'float']).columns\n",
    "print(f'Category Cols to encode: {COLS_TO_CAST}')\n",
    "print(f'Numerical Cols to scale: {COLS_TO_SCALE}')\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data\n",
    "- Training Set (80% of total): \n",
    "  - Used to train the models.\n",
    "- Validation Set (20% of Traning Set ): \n",
    "  - Used to fine-tune hyperparameters, select models, and monitor training progress.  \n",
    "- Testing Set (20% of total): \n",
    "  - Used to evaluate the final model's performance on unseen data and estimate its generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (49114, 27)\n",
      "X_validate shape: (12279, 27)\n",
      "X_test shape: (15349, 27)\n",
      "y_train shape: (49114,)\n",
      "y_validate shape: (12279,)\n",
      "y_test shape: (15349,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SPLIT = .2\n",
    "TRAINING_SPLIT = 1-TEST_SPLIT\n",
    "VALIDATION_SPLIT = .2\n",
    "\n",
    "# Split data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=85100)\n",
    "\n",
    "# Split the Training AGAIN into train and Validate\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=TEST_SPLIT, random_state=85100)\n",
    "\n",
    "# Then, you can use X_train and y_train for model training and X_test and y_test for evaluation.\n",
    "\n",
    "data_subset_dict = {\n",
    "    'X_train': X_train,\n",
    "    'X_validate': X_validate,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_validate': y_validate,\n",
    "    'y_test': y_test}\n",
    "\n",
    "for key, value in data_subset_dict.items():\n",
    "    shape = value.shape\n",
    "    print(f\"{key} shape: {shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### \n",
    "## SCALE X_train and X_validate\n",
    "#########\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "                ('num', StandardScaler(), COLS_TO_SCALE)\n",
    "                ],\n",
    "    remainder='passthrough')  # Leaves the rest of the columns alone\n",
    "\n",
    "# Fit on the training data\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transform the training and validation data\n",
    "X_train_scaled = preprocessor.transform(X_train)\n",
    "X_validate_scaled = preprocessor.transform(X_validate)\n",
    "\n",
    "# Now X_train_scaled and X_validate_scaled have the specified columns scaled, and the rest are unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared of base model: -2.904623057685253e+18\n",
      "RMSE of the base model: 3.855\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "#\n",
    "#  SIMPLE LINEAR REGRESSION Pipeline -\n",
    "#  -- No tuning. \n",
    "########################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "categorical_transform = Pipeline([('one-hot-encode', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))])\n",
    "\n",
    "preprocessing_df = ColumnTransformer([('categorical', categorical_transform, COLS_TO_CAST)])\n",
    "\n",
    "pipeline_base = Pipeline([('preprocessing', preprocessing_df),\n",
    "                          ('model', LinearRegression())])\n",
    "pipeline_base.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline_base.predict(X_validate)\n",
    "r2 = pipeline_base.score(X_validate, y_validate)\n",
    "\n",
    "mse = mean_squared_error(y_validate, y_pred, squared=False)\n",
    "print(f'R-squared of base model: {r2}')\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods - Baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<catboost.core.CatBoostRegressor object at 0x000002C319E4D090>: R2: 0.52, RMSE: 9515250466.18\n",
      "____________________________________________________________________________________________________\n",
      "Linear Regression: RMSE: 9515250466.184\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "#\n",
    "#  Ensemble Pipeline -\n",
    "#  -- No tuning. Change the variable \"model_name\" for other models. \n",
    "########################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# model_list= [LinearRegression(),ExtraTreesRegressor (),RandomForestRegressor(),XGBRegressor()]\n",
    "# model_names = [\"Linear Regression\",\"ExtraTreesRegressor\", \"Random Forest\",\"XGBRegressor\"]\n",
    "\n",
    "model_list= [LinearRegression()]\n",
    "model_names = [\"Linear Regression\"]\n",
    "\n",
    "ModScores = {}\n",
    "\n",
    "categorical_transform = Pipeline([('one-hot-encode', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))])\n",
    "\n",
    "preprocessing_df = ColumnTransformer([('categorical', categorical_transform, COLS_TO_CAST)])\n",
    "\n",
    "for model_names, model in zip(model_names, model_list):\n",
    "    pipeline_base = Pipeline([('preprocessing', preprocessing_df),\n",
    "                          ('model', LinearRegression())])\n",
    "    pipeline_base.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipeline_base.predict(X_validate)\n",
    "    \n",
    "    # Calculate the R-squared value\n",
    "    r2 = r2_score(y_validate, predictions)\n",
    "    rmse = mean_squared_error(y_validate, y_pred, squared=False)\n",
    "    \n",
    "    ModScores[model_names] = rmse\n",
    "    \n",
    "    print(f\"{model_name}: R2: {r2:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "print(\"_\"*100)\n",
    "for key, value in sorted(ModScores.items(), key=itemgetter(1), reverse=False):\n",
    "    print(f\"{key}: RMSE: {value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Model with Hyperparameter Tuning via Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "#  STANDALONE TUNING  - CatBoost\n",
    "# \n",
    "########################\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from catboost import Pool, CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "categorical_features_indices =[1,33,34,35]\n",
    "\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'iterations': [100, 200, 300],      # Number of boosting iterations\n",
    "    'depth': [6, 8, 10],                # Depth of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate  \n",
    "    }\n",
    "\n",
    "# Create a CatBoostRegressor model\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=catboost_model, param_grid=param_grid, cv=5, scoring=rmse_scorer, n_jobs=-1, error_score='raise')\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train, y_train, cat_features=categorical_features_indices)\n",
    "\n",
    "# Print the best hyperparameters and corresponding MSE score\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Best RMSE score:\", -grid_search.best_score_)\n",
    "\n",
    "# Get the best trained model\n",
    "best_catboost_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "validation_predictions = best_catboost_model.predict(X_validate)\n",
    "\n",
    "# Output - best settings for training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc4684d327e41d1a566380a3f441437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 5.4924213\ttest: 5.3998622\tbest: 5.3998622 (0)\ttotal: 274ms\tremaining: 54.5s\n",
      "10:\tlearn: 4.3908041\ttest: 4.3521760\tbest: 4.3521760 (10)\ttotal: 1.16s\tremaining: 19.9s\n",
      "20:\tlearn: 4.0655363\ttest: 4.1091377\tbest: 4.1091377 (20)\ttotal: 2.07s\tremaining: 17.6s\n",
      "30:\tlearn: 3.9183661\ttest: 4.0236519\tbest: 4.0236519 (30)\ttotal: 3.01s\tremaining: 16.4s\n",
      "40:\tlearn: 3.8108284\ttest: 3.9646639\tbest: 3.9646639 (40)\ttotal: 3.85s\tremaining: 15s\n",
      "50:\tlearn: 3.7525718\ttest: 3.9464708\tbest: 3.9464708 (50)\ttotal: 4.57s\tremaining: 13.4s\n",
      "60:\tlearn: 3.7046461\ttest: 3.9291717\tbest: 3.9291717 (60)\ttotal: 5.29s\tremaining: 12.1s\n",
      "70:\tlearn: 3.6728938\ttest: 3.9209391\tbest: 3.9209391 (70)\ttotal: 5.99s\tremaining: 10.9s\n",
      "80:\tlearn: 3.6356634\ttest: 3.9115113\tbest: 3.9115113 (80)\ttotal: 6.67s\tremaining: 9.8s\n",
      "90:\tlearn: 3.5977834\ttest: 3.9060306\tbest: 3.9060306 (90)\ttotal: 7.41s\tremaining: 8.88s\n",
      "100:\tlearn: 3.5684941\ttest: 3.9048493\tbest: 3.9048493 (100)\ttotal: 8.15s\tremaining: 7.98s\n",
      "110:\tlearn: 3.5428223\ttest: 3.8949711\tbest: 3.8949711 (110)\ttotal: 8.77s\tremaining: 7.03s\n",
      "120:\tlearn: 3.5168289\ttest: 3.8926423\tbest: 3.8920255 (116)\ttotal: 9.47s\tremaining: 6.18s\n",
      "130:\tlearn: 3.4980809\ttest: 3.8902582\tbest: 3.8893182 (128)\ttotal: 10.2s\tremaining: 5.39s\n",
      "140:\tlearn: 3.4717591\ttest: 3.8905800\tbest: 3.8893182 (128)\ttotal: 11s\tremaining: 4.58s\n",
      "150:\tlearn: 3.4460989\ttest: 3.8925059\tbest: 3.8893182 (128)\ttotal: 11.7s\tremaining: 3.79s\n",
      "160:\tlearn: 3.4250914\ttest: 3.8907121\tbest: 3.8893182 (128)\ttotal: 12.5s\tremaining: 3.03s\n",
      "170:\tlearn: 3.3992302\ttest: 3.8839331\tbest: 3.8839331 (170)\ttotal: 13.5s\tremaining: 2.29s\n",
      "180:\tlearn: 3.3660098\ttest: 3.8723351\tbest: 3.8723351 (180)\ttotal: 14.3s\tremaining: 1.5s\n",
      "190:\tlearn: 3.3296603\ttest: 3.8596909\tbest: 3.8596791 (188)\ttotal: 15.1s\tremaining: 712ms\n",
      "199:\tlearn: 3.3096737\ttest: 3.8557555\tbest: 3.8550405 (195)\ttotal: 15.8s\tremaining: 0us\n",
      "\n",
      "bestTest = 3.855040513\n",
      "bestIteration = 195\n",
      "\n",
      "Shrink model to first 196 iterations.\n",
      "R-squared of base model: 0.5232318591945018\n",
      "RMSE of the base model: 3.855\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "#\n",
    "#  OPTIMIZED - CatBoost\n",
    "# \n",
    "########################\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import Pool\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "###############\n",
    "# Specify categorical feature indices\n",
    "categorical_features_indices = [0, 2, 25, 26]\n",
    "\n",
    "\n",
    "# Create the Pool for training data\n",
    "train_pool = Pool(data=X_train, label=y_train, cat_features=categorical_features_indices)\n",
    "\n",
    "# If you have a validation dataset\n",
    "validation_pool = Pool(data=X_validate, label=y_validate, cat_features=categorical_features_indices)\n",
    "\n",
    "#########\n",
    "# Best hyperparameters found (after hyperparameter tuning)\n",
    "best_params = {'depth': 8, 'iterations': 200, 'learning_rate': 0.1}\n",
    "\n",
    "# Instantiate CatBoostRegressor with the best hyperparameters\n",
    "cat_model = CatBoostRegressor(**best_params)\n",
    "\n",
    "cat_model.fit(\n",
    "    train_pool,\n",
    "    eval_set=validation_pool,  # Remove this if you don't have a validation set\n",
    "    verbose=10,  # This will print the progress every 10 iterations\n",
    "    plot=True    # This will plot the learning curve (only works in Jupyter notebooks)\n",
    ")\n",
    "\n",
    "\n",
    "predictions = cat_model.predict(X_validate)  # If you used Pool, the data here should not be the Pool object but raw data.\n",
    "\n",
    "\n",
    "# Calculate the R-squared value\n",
    "r2 = r2_score(y_validate, predictions)\n",
    "\n",
    "## TODO:Fix RMSE in the baselineS! \n",
    "\n",
    "rmse = mean_squared_error(y_validate, predictions, squared=False)\n",
    "print(f'R-squared of base model: {r2}')\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lighthouse_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
