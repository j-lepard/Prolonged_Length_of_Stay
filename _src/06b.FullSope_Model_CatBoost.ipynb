{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from operator import itemgetter    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data from Pre-Processing\n",
    "* Missing values HAVE been imputed.\n",
    "* No PCA performed yet, no 1hot encoding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# Import Data from PreProcessing\n",
    "#\n",
    "#####################\n",
    "\n",
    "df= pd.read_csv('../_data/operations_imputed_CLEAN_v2.csv', index_col=0)\n",
    "\n",
    "df.drop(['race'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 76742 entries, 8 to 128030\n",
      "Data columns (total 37 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   op_id            76742 non-null  int64  \n",
      " 1   subject_id       76742 non-null  int64  \n",
      " 2   hadm_id          76742 non-null  int64  \n",
      " 3   opdate           76742 non-null  int64  \n",
      " 4   age              76742 non-null  int64  \n",
      " 5   sex              76742 non-null  object \n",
      " 6   weight           76742 non-null  float64\n",
      " 7   height           76742 non-null  float64\n",
      " 8   asa              76742 non-null  float64\n",
      " 9   department       76742 non-null  object \n",
      " 10  antype           76742 non-null  object \n",
      " 11  icd10_pcs        76742 non-null  object \n",
      " 12  category_desc    76742 non-null  object \n",
      " 13  desc_short       76742 non-null  object \n",
      " 14  category_id      76742 non-null  object \n",
      " 15  hr               76742 non-null  float64\n",
      " 16  pip              76742 non-null  float64\n",
      " 17  pmean            76742 non-null  float64\n",
      " 18  rr               76742 non-null  float64\n",
      " 19  spo2             76742 non-null  float64\n",
      " 20  vt               76742 non-null  float64\n",
      " 21  chloride         76742 non-null  float64\n",
      " 22  creatinine       76742 non-null  float64\n",
      " 23  glucose          76742 non-null  float64\n",
      " 24  hb               76742 non-null  float64\n",
      " 25  hco3             76742 non-null  float64\n",
      " 26  lymphocyte       76742 non-null  float64\n",
      " 27  platelet         76742 non-null  float64\n",
      " 28  potassium        76742 non-null  float64\n",
      " 29  sodium           76742 non-null  float64\n",
      " 30  total_bilirubin  76742 non-null  float64\n",
      " 31  wbc              76742 non-null  float64\n",
      " 32  LOS              76742 non-null  float64\n",
      " 33  prolonged_LOS    76742 non-null  int64  \n",
      " 34  icu_visit        76742 non-null  int64  \n",
      " 35  or_duration      76742 non-null  float64\n",
      " 36  anesth_duration  76742 non-null  float64\n",
      "dtypes: float64(23), int64(7), object(7)\n",
      "memory usage: 22.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the X and y DataFrames\n",
    "\n",
    "  * create y\n",
    "  * create X (complete with all the features)\n",
    "  * drop the features we identified as not meeting impact threshold. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 76742 entries, 8 to 128030\n",
      "Data columns (total 27 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   category_id      76742 non-null  object \n",
      " 1   age              76742 non-null  int64  \n",
      " 2   sex              76742 non-null  object \n",
      " 3   weight           76742 non-null  float64\n",
      " 4   height           76742 non-null  float64\n",
      " 5   hr               76742 non-null  float64\n",
      " 6   pip              76742 non-null  float64\n",
      " 7   pmean            76742 non-null  float64\n",
      " 8   rr               76742 non-null  float64\n",
      " 9   spo2             76742 non-null  float64\n",
      " 10  vt               76742 non-null  float64\n",
      " 11  chloride         76742 non-null  float64\n",
      " 12  creatinine       76742 non-null  float64\n",
      " 13  glucose          76742 non-null  float64\n",
      " 14  hb               76742 non-null  float64\n",
      " 15  hco3             76742 non-null  float64\n",
      " 16  lymphocyte       76742 non-null  float64\n",
      " 17  platelet         76742 non-null  float64\n",
      " 18  potassium        76742 non-null  float64\n",
      " 19  sodium           76742 non-null  float64\n",
      " 20  total_bilirubin  76742 non-null  float64\n",
      " 21  wbc              76742 non-null  float64\n",
      " 22  icu_visit        76742 non-null  int64  \n",
      " 23  or_duration      76742 non-null  float64\n",
      " 24  anesth_duration  76742 non-null  float64\n",
      " 25  department       76742 non-null  object \n",
      " 26  antype           76742 non-null  object \n",
      "dtypes: float64(21), int64(2), object(4)\n",
      "memory usage: 16.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# When doing a Categorical Model, reinsert 'prolonged_LOS' and instead, drop 'LOS'\n",
    "\n",
    "## Features to retain are those in X that will be used in training. Exludued features are features such as Operation_ID, Subject_ID..\n",
    "features_to_retain = ['category_id','age','sex',\t'weight',\t'height',\t'hr',\t'pip',\t'pmean',\t'rr',\t'spo2',\t'vt',\t'chloride',\t'creatinine',\t'glucose',\t'hb',\t'hco3',\t'lymphocyte',\t'platelet',\t'potassium',\t'sodium',\t'total_bilirubin',\t'wbc',\t'icu_visit',\t'or_duration',\t'anesth_duration',\t'department','antype'] \n",
    "\n",
    "## Create the Y, the Target\n",
    "y = df['LOS']\n",
    "\n",
    "## Create X the Features for Train/Test/Validate\n",
    "X = df.drop('LOS', axis=1)\n",
    "X= X[features_to_retain]\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Cols to encode: ['category_id', 'antype', 'sex', 'department']\n",
      "Numerical Cols to scale: Index(['age', 'weight', 'height', 'hr', 'pip', 'pmean', 'rr', 'spo2', 'vt',\n",
      "       'chloride', 'creatinine', 'glucose', 'hb', 'hco3', 'lymphocyte',\n",
      "       'platelet', 'potassium', 'sodium', 'total_bilirubin', 'wbc',\n",
      "       'icu_visit', 'or_duration', 'anesth_duration'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 76742 entries, 8 to 128030\n",
      "Data columns (total 27 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   category_id      76742 non-null  object \n",
      " 1   age              76742 non-null  int64  \n",
      " 2   sex              76742 non-null  object \n",
      " 3   weight           76742 non-null  float64\n",
      " 4   height           76742 non-null  float64\n",
      " 5   hr               76742 non-null  float64\n",
      " 6   pip              76742 non-null  float64\n",
      " 7   pmean            76742 non-null  float64\n",
      " 8   rr               76742 non-null  float64\n",
      " 9   spo2             76742 non-null  float64\n",
      " 10  vt               76742 non-null  float64\n",
      " 11  chloride         76742 non-null  float64\n",
      " 12  creatinine       76742 non-null  float64\n",
      " 13  glucose          76742 non-null  float64\n",
      " 14  hb               76742 non-null  float64\n",
      " 15  hco3             76742 non-null  float64\n",
      " 16  lymphocyte       76742 non-null  float64\n",
      " 17  platelet         76742 non-null  float64\n",
      " 18  potassium        76742 non-null  float64\n",
      " 19  sodium           76742 non-null  float64\n",
      " 20  total_bilirubin  76742 non-null  float64\n",
      " 21  wbc              76742 non-null  float64\n",
      " 22  icu_visit        76742 non-null  int64  \n",
      " 23  or_duration      76742 non-null  float64\n",
      " 24  anesth_duration  76742 non-null  float64\n",
      " 25  department       76742 non-null  object \n",
      " 26  antype           76742 non-null  object \n",
      "dtypes: float64(21), int64(2), object(4)\n",
      "memory usage: 16.4+ MB\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "#\n",
    "#  Indentify the columns that need to be either cast as Str or Scaled\n",
    "#\n",
    "############################################################\n",
    "\n",
    "#Category Columns that will need encoding. Cast them as String\n",
    "COLS_TO_CAST = ['category_id','antype','sex','department']  #When restoring scope to full category list, add cat_id here.\n",
    "                                                            # Convert the object data type columns to string\n",
    "\n",
    "X[COLS_TO_CAST] = X[COLS_TO_CAST].astype(str)\n",
    "\n",
    "# Numerical Columns for Scaling. Filter columns with dtype 'numeric' for scaling later in the Pipleine\n",
    "COLS_TO_SCALE = X.select_dtypes(include=['int', 'float']).columns\n",
    "\n",
    "\n",
    "print(f'Category Cols to encode: {COLS_TO_CAST}')\n",
    "print(f'Numerical Cols to scale: {COLS_TO_SCALE}')\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Split\n",
    "\n",
    "- Training Set (80% of total): \n",
    "  - Used to train the models.\n",
    "- Validation Set (20% of Traning Set ): \n",
    "  - Used to fine-tune hyperparameters, select models, and monitor training progress.  \n",
    "- Testing Set (20% of total): \n",
    "  - Used to evaluate the final model's performance on unseen data and estimate its generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (49114, 27)\n",
      "X_validate shape: (12279, 27)\n",
      "X_test shape: (15349, 27)\n",
      "y_train shape: (49114,)\n",
      "y_validate shape: (12279,)\n",
      "y_test shape: (15349,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SPLIT = .2\n",
    "TRAINING_SPLIT = 1-TEST_SPLIT\n",
    "VALIDATION_SPLIT = .2\n",
    "\n",
    "# Split data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=85100)\n",
    "\n",
    "# Split the Training AGAIN into train and Validate\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=TEST_SPLIT, random_state=85100)\n",
    "\n",
    "# use X_train and y_train for model training and X_val and y_val for turning.\n",
    "\n",
    "data_subset_dict = {\n",
    "    'X_train': X_train,\n",
    "    'X_validate': X_validate,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_validate': y_validate,\n",
    "    'y_test': y_test}\n",
    "\n",
    "for key, value in data_subset_dict.items():\n",
    "    shape = value.shape\n",
    "    print(f\"{key} shape: {shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale X (Numerical)\n",
    "* We do NOT scale Y, the target\n",
    "* We fit the StandardScaler on X_training and then transform both your training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### \n",
    "## SCALE X_train and X_validate\n",
    "#########\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "                ('num', StandardScaler(), COLS_TO_SCALE)\n",
    "                ],\n",
    "    remainder='passthrough')  # Leaves the rest of the columns alone\n",
    "\n",
    "# Fit on the training data\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transform the training and validation data\n",
    "X_train_scaled = preprocessor.transform(X_train)\n",
    "X_validate_scaled = preprocessor.transform(X_validate)\n",
    "\n",
    "# Now X_train_scaled and X_validate_scaled have the specified columns scaled, and the rest are unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared of base model: -2.904623057685253e+18\n",
      "RMSE of the base model: 9515250466.184\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "#\n",
    "#  SIMPLE LINEAR REGRESSION Pipeline -\n",
    "#  -- No tuning. \n",
    "########################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "categorical_transform = Pipeline([('one-hot-encode', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))])\n",
    "\n",
    "preprocessing_df = ColumnTransformer([('categorical', categorical_transform, COLS_TO_CAST)])\n",
    "\n",
    "pipeline_base = Pipeline([('preprocessing', preprocessing_df),\n",
    "                          ('model', LinearRegression())])\n",
    "pipeline_base.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline_base.predict(X_validate)\n",
    "r2 = r2_score(y_validate, y_pred)\n",
    "rmse = mean_squared_error(y_validate, y_pred, squared=False)\n",
    "print(f'R-squared of base model: {r2}')\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;categorical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;one-hot-encode&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                                 sparse_output=False))]),\n",
       "                                                  [&#x27;category_id&#x27;, &#x27;antype&#x27;,\n",
       "                                                   &#x27;sex&#x27;, &#x27;department&#x27;])])),\n",
       "                (&#x27;model&#x27;, LinearRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;categorical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;one-hot-encode&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                                 sparse_output=False))]),\n",
       "                                                  [&#x27;category_id&#x27;, &#x27;antype&#x27;,\n",
       "                                                   &#x27;sex&#x27;, &#x27;department&#x27;])])),\n",
       "                (&#x27;model&#x27;, LinearRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessing: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;categorical&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;one-hot-encode&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse_output=False))]),\n",
       "                                 [&#x27;category_id&#x27;, &#x27;antype&#x27;, &#x27;sex&#x27;,\n",
       "                                  &#x27;department&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical</label><div class=\"sk-toggleable__content\"><pre>[&#x27;category_id&#x27;, &#x27;antype&#x27;, &#x27;sex&#x27;, &#x27;department&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 ColumnTransformer(transformers=[('categorical',\n",
       "                                                  Pipeline(steps=[('one-hot-encode',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                                 sparse_output=False))]),\n",
       "                                                  ['category_id', 'antype',\n",
       "                                                   'sex', 'department'])])),\n",
       "                ('model', LinearRegression())])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods - Baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "#  Ensemble Pipeline -\n",
    "#  -- No tuning. \n",
    "########################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "model_list= [LinearRegression(),ExtraTreesRegressor (n_jobs=-1),RandomForestRegressor(n_jobs=-1),XGBRegressor(n_jobs=-1)]\n",
    "model_names = [\"Linear Regression\",\"ExtraTreesRegressor\", \"Random Forest\",\"XGBRegressor\"]\n",
    "\n",
    "# model_list= [LinearRegression()]\n",
    "# model_names = [\"Linear Regression\"]\n",
    "\n",
    "ModScores = {}\n",
    "\n",
    "categorical_transform = Pipeline([('one-hot-encode', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))])\n",
    "\n",
    "preprocessing_df = ColumnTransformer([('categorical', categorical_transform, COLS_TO_CAST)])\n",
    "\n",
    "for model_names, model in zip(model_names, model_list):\n",
    "    pipeline_base = Pipeline([('preprocessing', preprocessing_df),\n",
    "                          ('model', model)])\n",
    "    pipeline_base.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipeline_base.predict(X_validate)\n",
    "    \n",
    "    # Calculate the R-squared value\n",
    "    r2 = r2_score(y_validate, y_pred)\n",
    "    rmse = mean_squared_error(y_validate, y_pred, squared=False)\n",
    "    \n",
    "    ModScores[model_names] = rmse\n",
    "    \n",
    "    print(f\"{model}: R2: {r2:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "print(\"_\"*100)\n",
    "for key, value in sorted(ModScores.items(), key=itemgetter(1), reverse=False):\n",
    "    print(f\"{key}: RMSE: {value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Model with Hyperparameter Tuning via Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "#  STANDALONE TUNING  - CatBoost\n",
    "# \n",
    "########################\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from catboost import Pool, CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "categorical_features_indices =[0, 2, 25, 26]\n",
    "\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'iterations': [100, 200, 300],      # Number of boosting iterations\n",
    "    'depth': [6, 8, 10],                # Depth of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate  \n",
    "    }\n",
    "\n",
    "# Create a CatBoostRegressor model\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=catboost_model, param_grid=param_grid, cv=5, scoring=rmse_scorer, n_jobs=-1, error_score='raise')\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train, y_train, cat_features=categorical_features_indices)\n",
    "\n",
    "# Print the best hyperparameters and corresponding MSE score\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Best RMSE score:\", -grid_search.best_score_)\n",
    "\n",
    "# Get the best trained model\n",
    "best_catboost_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "validation_predictions = best_catboost_model.predict(X_validate)\n",
    "\n",
    "# Output - best settings for training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7798d6343b384d3e8a69e864ce27d092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 5.4924213\ttest: 5.3998622\tbest: 5.3998622 (0)\ttotal: 212ms\tremaining: 42.2s\n",
      "10:\tlearn: 4.3908041\ttest: 4.3521760\tbest: 4.3521760 (10)\ttotal: 909ms\tremaining: 15.6s\n",
      "20:\tlearn: 4.0655363\ttest: 4.1091377\tbest: 4.1091377 (20)\ttotal: 1.63s\tremaining: 13.9s\n",
      "30:\tlearn: 3.9183661\ttest: 4.0236519\tbest: 4.0236519 (30)\ttotal: 2.37s\tremaining: 12.9s\n",
      "40:\tlearn: 3.8108284\ttest: 3.9646639\tbest: 3.9646639 (40)\ttotal: 3.17s\tremaining: 12.3s\n",
      "50:\tlearn: 3.7525718\ttest: 3.9464708\tbest: 3.9464708 (50)\ttotal: 3.94s\tremaining: 11.5s\n",
      "60:\tlearn: 3.7046461\ttest: 3.9291717\tbest: 3.9291717 (60)\ttotal: 4.67s\tremaining: 10.6s\n",
      "70:\tlearn: 3.6728938\ttest: 3.9209391\tbest: 3.9209391 (70)\ttotal: 5.37s\tremaining: 9.76s\n",
      "80:\tlearn: 3.6356634\ttest: 3.9115113\tbest: 3.9115113 (80)\ttotal: 6.06s\tremaining: 8.9s\n",
      "90:\tlearn: 3.5977834\ttest: 3.9060306\tbest: 3.9060306 (90)\ttotal: 6.8s\tremaining: 8.14s\n",
      "100:\tlearn: 3.5684941\ttest: 3.9048493\tbest: 3.9048493 (100)\ttotal: 7.58s\tremaining: 7.42s\n",
      "110:\tlearn: 3.5428223\ttest: 3.8949711\tbest: 3.8949711 (110)\ttotal: 8.25s\tremaining: 6.62s\n",
      "120:\tlearn: 3.5168289\ttest: 3.8926423\tbest: 3.8920255 (116)\ttotal: 9s\tremaining: 5.87s\n",
      "130:\tlearn: 3.4980809\ttest: 3.8902582\tbest: 3.8893182 (128)\ttotal: 9.78s\tremaining: 5.15s\n",
      "140:\tlearn: 3.4717591\ttest: 3.8905800\tbest: 3.8893182 (128)\ttotal: 10.6s\tremaining: 4.43s\n",
      "150:\tlearn: 3.4460989\ttest: 3.8925059\tbest: 3.8893182 (128)\ttotal: 11.4s\tremaining: 3.69s\n",
      "160:\tlearn: 3.4250914\ttest: 3.8907121\tbest: 3.8893182 (128)\ttotal: 12.2s\tremaining: 2.95s\n",
      "170:\tlearn: 3.3992302\ttest: 3.8839331\tbest: 3.8839331 (170)\ttotal: 13.1s\tremaining: 2.21s\n",
      "180:\tlearn: 3.3660098\ttest: 3.8723351\tbest: 3.8723351 (180)\ttotal: 13.8s\tremaining: 1.44s\n",
      "190:\tlearn: 3.3296603\ttest: 3.8596909\tbest: 3.8596791 (188)\ttotal: 14.5s\tremaining: 683ms\n",
      "199:\tlearn: 3.3096737\ttest: 3.8557555\tbest: 3.8550405 (195)\ttotal: 15.2s\tremaining: 0us\n",
      "\n",
      "bestTest = 3.855040513\n",
      "bestIteration = 195\n",
      "\n",
      "Shrink model to first 196 iterations.\n",
      "R-squared of base model: 0.5232318591945018\n",
      "RMSE of the base model: 3.855\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "#\n",
    "#  OPTIMIZED - CatBoost\n",
    "# \n",
    "########################\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import Pool\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "###############\n",
    "# Specify categorical feature indices\n",
    "categorical_features_indices = [0, 2, 25, 26]\n",
    "\n",
    "\n",
    "# Create the Pool for training data\n",
    "train_pool = Pool(data=X_train, label=y_train, cat_features=categorical_features_indices)\n",
    "\n",
    "# If you have a validation dataset\n",
    "validation_pool = Pool(data=X_validate, label=y_validate, cat_features=categorical_features_indices)\n",
    "\n",
    "#########\n",
    "# Best hyperparameters found (after hyperparameter tuning)\n",
    "best_params = {'depth': 8, 'iterations': 200, 'learning_rate': 0.1}\n",
    "\n",
    "# Instantiate CatBoostRegressor with the best hyperparameters\n",
    "cat_model = CatBoostRegressor(**best_params)\n",
    "\n",
    "cat_model.fit(\n",
    "    train_pool,\n",
    "    eval_set=validation_pool,  # Remove this if you don't have a validation set\n",
    "    verbose=10,  # This will print the progress every 10 iterations\n",
    "    plot=True    # This will plot the learning curve (only works in Jupyter notebooks)\n",
    ")\n",
    "\n",
    "\n",
    "predictions = cat_model.predict(X_validate)  # If you used Pool, the data here should not be the Pool object but raw data.\n",
    "\n",
    "\n",
    "# Calculate the R-squared value\n",
    "r2 = r2_score(y_validate, predictions)\n",
    "rmse = mean_squared_error(y_validate, predictions, squared=False)\n",
    "\n",
    "\n",
    "print(f'R-squared of base model: {r2}')\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lighthouse_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
