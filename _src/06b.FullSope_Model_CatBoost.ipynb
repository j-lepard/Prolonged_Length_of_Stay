{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from operator import itemgetter    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data from Pre-Processing\n",
    "* In this scenario, we are using only the surgeries with the greatest volume count (id_category: 08R). \n",
    "* Missing values HAVE been imputed.\n",
    "* No PCA performed yet, no 1hot encoding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# Trial with SINGLE Operation Category\n",
    "#\n",
    "#####################\n",
    "\n",
    "df= pd.read_csv('../_data/operations_imputed_CLEAN_v2.csv', index_col=0)\n",
    "\n",
    "# Going for it - doing the whole deal\n",
    "# df = df[df['category_id']=='08R']\n",
    "df.drop(['race'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 76742 entries, 8 to 128030\n",
      "Data columns (total 37 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   op_id            76742 non-null  int64  \n",
      " 1   subject_id       76742 non-null  int64  \n",
      " 2   hadm_id          76742 non-null  int64  \n",
      " 3   opdate           76742 non-null  int64  \n",
      " 4   age              76742 non-null  int64  \n",
      " 5   sex              76742 non-null  object \n",
      " 6   weight           76742 non-null  float64\n",
      " 7   height           76742 non-null  float64\n",
      " 8   asa              76742 non-null  float64\n",
      " 9   department       76742 non-null  object \n",
      " 10  antype           76742 non-null  object \n",
      " 11  icd10_pcs        76742 non-null  object \n",
      " 12  category_desc    76742 non-null  object \n",
      " 13  desc_short       76742 non-null  object \n",
      " 14  category_id      76742 non-null  object \n",
      " 15  hr               76742 non-null  float64\n",
      " 16  pip              76742 non-null  float64\n",
      " 17  pmean            76742 non-null  float64\n",
      " 18  rr               76742 non-null  float64\n",
      " 19  spo2             76742 non-null  float64\n",
      " 20  vt               76742 non-null  float64\n",
      " 21  chloride         76742 non-null  float64\n",
      " 22  creatinine       76742 non-null  float64\n",
      " 23  glucose          76742 non-null  float64\n",
      " 24  hb               76742 non-null  float64\n",
      " 25  hco3             76742 non-null  float64\n",
      " 26  lymphocyte       76742 non-null  float64\n",
      " 27  platelet         76742 non-null  float64\n",
      " 28  potassium        76742 non-null  float64\n",
      " 29  sodium           76742 non-null  float64\n",
      " 30  total_bilirubin  76742 non-null  float64\n",
      " 31  wbc              76742 non-null  float64\n",
      " 32  LOS              76742 non-null  float64\n",
      " 33  prolonged_LOS    76742 non-null  int64  \n",
      " 34  icu_visit        76742 non-null  int64  \n",
      " 35  or_duration      76742 non-null  float64\n",
      " 36  anesth_duration  76742 non-null  float64\n",
      "dtypes: float64(23), int64(7), object(7)\n",
      "memory usage: 22.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the X and y DataFrames\n",
    "\n",
    "  * create y\n",
    "  * create X (complete with all the features)\n",
    "  * drop the features we identified as not meeting impact threshold. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 76742 entries, 8 to 128030\n",
      "Data columns (total 28 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   category_id      76742 non-null  object \n",
      " 1   age              76742 non-null  int64  \n",
      " 2   sex              76742 non-null  object \n",
      " 3   weight           76742 non-null  float64\n",
      " 4   height           76742 non-null  float64\n",
      " 5   hr               76742 non-null  float64\n",
      " 6   pip              76742 non-null  float64\n",
      " 7   pmean            76742 non-null  float64\n",
      " 8   rr               76742 non-null  float64\n",
      " 9   spo2             76742 non-null  float64\n",
      " 10  vt               76742 non-null  float64\n",
      " 11  chloride         76742 non-null  float64\n",
      " 12  creatinine       76742 non-null  float64\n",
      " 13  glucose          76742 non-null  float64\n",
      " 14  hb               76742 non-null  float64\n",
      " 15  hco3             76742 non-null  float64\n",
      " 16  lymphocyte       76742 non-null  float64\n",
      " 17  platelet         76742 non-null  float64\n",
      " 18  potassium        76742 non-null  float64\n",
      " 19  sodium           76742 non-null  float64\n",
      " 20  total_bilirubin  76742 non-null  float64\n",
      " 21  wbc              76742 non-null  float64\n",
      " 22  icu_visit        76742 non-null  int64  \n",
      " 23  or_duration      76742 non-null  float64\n",
      " 24  anesth_duration  76742 non-null  float64\n",
      " 25  asa              76742 non-null  float64\n",
      " 26  department       76742 non-null  object \n",
      " 27  antype           76742 non-null  object \n",
      "dtypes: float64(22), int64(2), object(4)\n",
      "memory usage: 17.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Label = LOS\n",
    "\n",
    "# When doing a Categorical Model, reinsert 'prolonged_LOS' and instead, drop 'LOS'\n",
    "\n",
    "features_to_retain = ['category_id','age','sex',\t'weight',\t'height',\t'hr',\t'pip',\t'pmean',\t'rr',\t'spo2',\t'vt',\t'chloride',\t'creatinine',\t'glucose',\t'hb',\t'hco3',\t'lymphocyte',\t'platelet',\t'potassium',\t'sodium',\t'total_bilirubin',\t'wbc',\t'icu_visit',\t'or_duration',\t'anesth_duration',\t'asa','department','antype'] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y = df['LOS']\n",
    "X = df.drop('LOS', axis=1)\n",
    "# Get a list of column names with data type 'object'\n",
    "\n",
    "\n",
    "X= X[features_to_retain]\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Cols to encode: ['category_id', 'antype', 'asa', 'sex', 'department']\n",
      "Numerical Cols to scale: Index(['age', 'weight', 'height', 'hr', 'pip', 'pmean', 'rr', 'spo2', 'vt',\n",
      "       'chloride', 'creatinine', 'glucose', 'hb', 'hco3', 'lymphocyte',\n",
      "       'platelet', 'potassium', 'sodium', 'total_bilirubin', 'wbc',\n",
      "       'icu_visit', 'or_duration', 'anesth_duration'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 76742 entries, 8 to 128030\n",
      "Data columns (total 28 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   category_id      76742 non-null  object \n",
      " 1   age              76742 non-null  int64  \n",
      " 2   sex              76742 non-null  object \n",
      " 3   weight           76742 non-null  float64\n",
      " 4   height           76742 non-null  float64\n",
      " 5   hr               76742 non-null  float64\n",
      " 6   pip              76742 non-null  float64\n",
      " 7   pmean            76742 non-null  float64\n",
      " 8   rr               76742 non-null  float64\n",
      " 9   spo2             76742 non-null  float64\n",
      " 10  vt               76742 non-null  float64\n",
      " 11  chloride         76742 non-null  float64\n",
      " 12  creatinine       76742 non-null  float64\n",
      " 13  glucose          76742 non-null  float64\n",
      " 14  hb               76742 non-null  float64\n",
      " 15  hco3             76742 non-null  float64\n",
      " 16  lymphocyte       76742 non-null  float64\n",
      " 17  platelet         76742 non-null  float64\n",
      " 18  potassium        76742 non-null  float64\n",
      " 19  sodium           76742 non-null  float64\n",
      " 20  total_bilirubin  76742 non-null  float64\n",
      " 21  wbc              76742 non-null  float64\n",
      " 22  icu_visit        76742 non-null  int64  \n",
      " 23  or_duration      76742 non-null  float64\n",
      " 24  anesth_duration  76742 non-null  float64\n",
      " 25  asa              76742 non-null  object \n",
      " 26  department       76742 non-null  object \n",
      " 27  antype           76742 non-null  object \n",
      "dtypes: float64(21), int64(2), object(5)\n",
      "memory usage: 17.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Indentify the columns that need to be either cast as Str or Scaled\n",
    "############################################################\n",
    "\n",
    "COLS_TO_CAST = ['category_id','antype','asa','sex','department'] #When restoring scope to full category list, add cat_id here.\n",
    "# Convert the object data type columns to string\n",
    "\n",
    "X[COLS_TO_CAST] = X[COLS_TO_CAST].astype(str)\n",
    "\n",
    "# Filter columns with dtype 'numeric' for scaling later in the Pipleine\n",
    "COLS_TO_SCALE = X.select_dtypes(include=['int', 'float']).columns\n",
    "print(f'Category Cols to encode: {COLS_TO_CAST}')\n",
    "print(f'Numerical Cols to scale: {COLS_TO_SCALE}')\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data\n",
    "- Training Set (80% of total): \n",
    "  - Used to train the models.\n",
    "- Validation Set (20% of Traning Set ): \n",
    "  - Used to fine-tune hyperparameters, select models, and monitor training progress.  \n",
    "- Testing Set (20% of total): \n",
    "  - Used to evaluate the final model's performance on unseen data and estimate its generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (49114, 28)\n",
      "X_validate shape: (12279, 28)\n",
      "X_test shape: (15349, 28)\n",
      "y_train shape: (49114,)\n",
      "y_validate shape: (12279,)\n",
      "y_test shape: (15349,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SPLIT = .2\n",
    "TRAINING_SPLIT = 1-TEST_SPLIT\n",
    "VALIDATION_SPLIT = .2\n",
    "\n",
    "# Split data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=85100)\n",
    "\n",
    "# Split the Training AGAIN into train and Validate\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=TEST_SPLIT, random_state=85100)\n",
    "\n",
    "# Then, you can use X_train and y_train for model training and X_test and y_test for evaluation.\n",
    "\n",
    "data_subset_dict = {\n",
    "    'X_train': X_train,\n",
    "    'X_validate': X_validate,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_validate': y_validate,\n",
    "    'y_test': y_test}\n",
    "\n",
    "for key, value in data_subset_dict.items():\n",
    "    shape = value.shape\n",
    "    print(f\"{key} shape: {shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 49114 entries, 124497 to 82329\n",
      "Data columns (total 28 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   category_id      49114 non-null  object \n",
      " 1   age              49114 non-null  int64  \n",
      " 2   sex              49114 non-null  object \n",
      " 3   weight           49114 non-null  float64\n",
      " 4   height           49114 non-null  float64\n",
      " 5   hr               49114 non-null  float64\n",
      " 6   pip              49114 non-null  float64\n",
      " 7   pmean            49114 non-null  float64\n",
      " 8   rr               49114 non-null  float64\n",
      " 9   spo2             49114 non-null  float64\n",
      " 10  vt               49114 non-null  float64\n",
      " 11  chloride         49114 non-null  float64\n",
      " 12  creatinine       49114 non-null  float64\n",
      " 13  glucose          49114 non-null  float64\n",
      " 14  hb               49114 non-null  float64\n",
      " 15  hco3             49114 non-null  float64\n",
      " 16  lymphocyte       49114 non-null  float64\n",
      " 17  platelet         49114 non-null  float64\n",
      " 18  potassium        49114 non-null  float64\n",
      " 19  sodium           49114 non-null  float64\n",
      " 20  total_bilirubin  49114 non-null  float64\n",
      " 21  wbc              49114 non-null  float64\n",
      " 22  icu_visit        49114 non-null  int64  \n",
      " 23  or_duration      49114 non-null  float64\n",
      " 24  anesth_duration  49114 non-null  float64\n",
      " 25  asa              49114 non-null  object \n",
      " 26  department       49114 non-null  object \n",
      " 27  antype           49114 non-null  object \n",
      "dtypes: float64(21), int64(2), object(5)\n",
      "memory usage: 10.9+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "#  SIMPLE LINEAR REGRESSION Pipeline -\n",
    "#  -- No tuning. \n",
    "########################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "numeric_transform = Pipeline([('impute_mean', SimpleImputer(strategy='mean')),\n",
    "                              ('scaling', StandardScaler())])\n",
    "\n",
    "categorical_transform = Pipeline([('impute_mode', SimpleImputer(strategy='most_frequent')),\n",
    "                                  ('one-hot-encode', OneHotEncoder(sparse_output=False))])\n",
    "\n",
    "preprocessing_df = ColumnTransformer([('numeric', numeric_transform, ['age','weight',\t'height',\t'art_mbp',\t'art_sbp',\t'bt',\t'cvp',\t'hr',\t'pip',\t'pmean',\t'rr',\t'spo2',\t'vt',\t'alp',\t'alt',\t'ast',\t'chloride',\t'creatinine',\t'glucose',\t'hb',\t'hco3',\t'lymphocyte',\t'platelet',\t'potassium',\t'sodium',\t'total_bilirubin',\t'wbc',\t'is_outlier',\t'prolonged_LOS',\t'icu_visit',\t'or_duration',\t'anesth_duration']),\n",
    "                                      ('categorical', categorical_transform, ['sex','asa','department','antype'])])\n",
    "\n",
    "\n",
    "pipeline_base = Pipeline([('proprocessing', preprocessing_df),\n",
    "                    ('model', LinearRegression())])\n",
    "pipeline_base.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = pipeline_base.predict(X_validate)\n",
    "r2 = pipeline_base.score(X_validate, y_validate)\n",
    "\n",
    "mse = mean_squared_error(y_validate, y_pred, squared=False)\n",
    "rmse = sqrt(mse)\n",
    "print(f'R-squared of base model: {r2}')\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "#  Ensemble Pipeline -\n",
    "#  -- No tuning. Change the variable \"model_name\" for other models. \n",
    "########################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Model with Hyperparameter Tuning via Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "#  STANDALONE TUNING  - CatBoost\n",
    "# \n",
    "########################\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from catboost import Pool, CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def rmse_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    rmse = sqrt(mse)\n",
    "    return -rmse  # Return negative RMSE for grid search to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "categorical_features_indices =[1,33,34,35]\n",
    "\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'iterations': [100, 200, 300],      # Number of boosting iterations\n",
    "    'depth': [6, 8, 10],                # Depth of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate  \n",
    "    }\n",
    "\n",
    "# Create a CatBoostRegressor model\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=catboost_model, param_grid=param_grid, cv=5, scoring=rmse_scorer, n_jobs=-1, error_score='raise')\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train, y_train, cat_features=categorical_features_indices)\n",
    "\n",
    "# Print the best hyperparameters and corresponding MSE score\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Best RMSE score:\", -grid_search.best_score_)\n",
    "\n",
    "# Get the best trained model\n",
    "best_catboost_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "validation_predictions = best_catboost_model.predict(X_validate)\n",
    "\n",
    "# Output - best settings for training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "CatBoostError",
     "evalue": "'data' is numpy array of floating point numerical type, it means no categorical features, but 'cat_features' parameter specifies nonzero number of categorical features",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jamie\\OneDrive - Lepard & Lepard\\Data Science\\LighthouseLabs\\Python_Projects\\Prolonged_LOS_Project\\_src\\06b.FullSope_Model_CatBoost.ipynb Cell 24\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamie/OneDrive%20-%20Lepard%20%26%20Lepard/Data%20Science/LighthouseLabs/Python_Projects/Prolonged_LOS_Project/_src/06b.FullSope_Model_CatBoost.ipynb#X32sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m preprocessing_df \u001b[39m=\u001b[39m ColumnTransformer([(\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m, numeric_transform, COLS_TO_SCALE),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamie/OneDrive%20-%20Lepard%20%26%20Lepard/Data%20Science/LighthouseLabs/Python_Projects/Prolonged_LOS_Project/_src/06b.FullSope_Model_CatBoost.ipynb#X32sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                                       (\u001b[39m'\u001b[39m\u001b[39mcategorical\u001b[39m\u001b[39m'\u001b[39m, categorical_transform, COLS_TO_SCALE)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamie/OneDrive%20-%20Lepard%20%26%20Lepard/Data%20Science/LighthouseLabs/Python_Projects/Prolonged_LOS_Project/_src/06b.FullSope_Model_CatBoost.ipynb#X32sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m pipeline_base \u001b[39m=\u001b[39m Pipeline([(\u001b[39m'\u001b[39m\u001b[39mproprocessing\u001b[39m\u001b[39m'\u001b[39m, preprocessing_df),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamie/OneDrive%20-%20Lepard%20%26%20Lepard/Data%20Science/LighthouseLabs/Python_Projects/Prolonged_LOS_Project/_src/06b.FullSope_Model_CatBoost.ipynb#X32sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m                     (\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m, model_name)])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jamie/OneDrive%20-%20Lepard%20%26%20Lepard/Data%20Science/LighthouseLabs/Python_Projects/Prolonged_LOS_Project/_src/06b.FullSope_Model_CatBoost.ipynb#X32sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m pipeline_base\u001b[39m.\u001b[39;49mfit(X_train, y_train, model__cat_features\u001b[39m=\u001b[39;49mcategorical_features_indices)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamie/OneDrive%20-%20Lepard%20%26%20Lepard/Data%20Science/LighthouseLabs/Python_Projects/Prolonged_LOS_Project/_src/06b.FullSope_Model_CatBoost.ipynb#X32sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m y_pred \u001b[39m=\u001b[39m pipeline_base\u001b[39m.\u001b[39mpredict(X_validate)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamie/OneDrive%20-%20Lepard%20%26%20Lepard/Data%20Science/LighthouseLabs/Python_Projects/Prolonged_LOS_Project/_src/06b.FullSope_Model_CatBoost.ipynb#X32sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m r2 \u001b[39m=\u001b[39m pipeline_base\u001b[39m.\u001b[39mscore(X_validate, y_validate)\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\sklearn\\pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    419\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 420\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    422\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\catboost\\core.py:5703\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5700\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[0;32m   5701\u001b[0m     CatBoostRegressor\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m-> 5703\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline,\n\u001b[0;32m   5704\u001b[0m                  use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description,\n\u001b[0;32m   5705\u001b[0m                  verbose_eval, metric_period, silent, early_stopping_rounds,\n\u001b[0;32m   5706\u001b[0m                  save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\catboost\\core.py:2303\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2300\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(X, PATH_TYPES \u001b[39m+\u001b[39m (Pool,)):\n\u001b[0;32m   2301\u001b[0m     \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m\"\u001b[39m\u001b[39my may be None only when X is an instance of catboost.Pool or string\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2303\u001b[0m train_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_train_params(\n\u001b[0;32m   2304\u001b[0m     X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, cat_features\u001b[39m=\u001b[39;49mcat_features, text_features\u001b[39m=\u001b[39;49mtext_features, embedding_features\u001b[39m=\u001b[39;49membedding_features,\n\u001b[0;32m   2305\u001b[0m     pairs\u001b[39m=\u001b[39;49mpairs, sample_weight\u001b[39m=\u001b[39;49msample_weight, group_id\u001b[39m=\u001b[39;49mgroup_id, group_weight\u001b[39m=\u001b[39;49mgroup_weight,\n\u001b[0;32m   2306\u001b[0m     subgroup_id\u001b[39m=\u001b[39;49msubgroup_id, pairs_weight\u001b[39m=\u001b[39;49mpairs_weight, baseline\u001b[39m=\u001b[39;49mbaseline, use_best_model\u001b[39m=\u001b[39;49muse_best_model,\n\u001b[0;32m   2307\u001b[0m     eval_set\u001b[39m=\u001b[39;49meval_set, verbose\u001b[39m=\u001b[39;49mverbose, logging_level\u001b[39m=\u001b[39;49mlogging_level, plot\u001b[39m=\u001b[39;49mplot, plot_file\u001b[39m=\u001b[39;49mplot_file,\n\u001b[0;32m   2308\u001b[0m     column_description\u001b[39m=\u001b[39;49mcolumn_description, verbose_eval\u001b[39m=\u001b[39;49mverbose_eval, metric_period\u001b[39m=\u001b[39;49mmetric_period,\n\u001b[0;32m   2309\u001b[0m     silent\u001b[39m=\u001b[39;49msilent, early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds, save_snapshot\u001b[39m=\u001b[39;49msave_snapshot,\n\u001b[0;32m   2310\u001b[0m     snapshot_file\u001b[39m=\u001b[39;49msnapshot_file, snapshot_interval\u001b[39m=\u001b[39;49msnapshot_interval, init_model\u001b[39m=\u001b[39;49minit_model,\n\u001b[0;32m   2311\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks\n\u001b[0;32m   2312\u001b[0m )\n\u001b[0;32m   2313\u001b[0m params \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   2314\u001b[0m train_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mtrain_pool\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\catboost\\core.py:2184\u001b[0m, in \u001b[0;36mCatBoost._prepare_train_params\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks)\u001b[0m\n\u001b[0;32m   2181\u001b[0m text_features \u001b[39m=\u001b[39m _process_feature_indices(text_features, X, params, \u001b[39m'\u001b[39m\u001b[39mtext_features\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   2182\u001b[0m embedding_features \u001b[39m=\u001b[39m _process_feature_indices(embedding_features, X, params, \u001b[39m'\u001b[39m\u001b[39membedding_features\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 2184\u001b[0m train_pool \u001b[39m=\u001b[39m _build_train_pool(X, y, cat_features, text_features, embedding_features, pairs,\n\u001b[0;32m   2185\u001b[0m                                sample_weight, group_id, group_weight, subgroup_id, pairs_weight,\n\u001b[0;32m   2186\u001b[0m                                baseline, column_description)\n\u001b[0;32m   2187\u001b[0m \u001b[39mif\u001b[39;00m train_pool\u001b[39m.\u001b[39mis_empty_:\n\u001b[0;32m   2188\u001b[0m     \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m\"\u001b[39m\u001b[39mX is empty.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\catboost\\core.py:1444\u001b[0m, in \u001b[0;36m_build_train_pool\u001b[1;34m(X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, column_description)\u001b[0m\n\u001b[0;32m   1442\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1443\u001b[0m         \u001b[39mraise\u001b[39;00m CatBoostError(\u001b[39m\"\u001b[39m\u001b[39my has not initialized in fit(): X is not catboost.Pool object, y must be not None in fit().\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1444\u001b[0m     train_pool \u001b[39m=\u001b[39m Pool(X, y, cat_features\u001b[39m=\u001b[39;49mcat_features, text_features\u001b[39m=\u001b[39;49mtext_features, embedding_features\u001b[39m=\u001b[39;49membedding_features, pairs\u001b[39m=\u001b[39;49mpairs, weight\u001b[39m=\u001b[39;49msample_weight, group_id\u001b[39m=\u001b[39;49mgroup_id,\n\u001b[0;32m   1445\u001b[0m                       group_weight\u001b[39m=\u001b[39;49mgroup_weight, subgroup_id\u001b[39m=\u001b[39;49msubgroup_id, pairs_weight\u001b[39m=\u001b[39;49mpairs_weight, baseline\u001b[39m=\u001b[39;49mbaseline)\n\u001b[0;32m   1446\u001b[0m \u001b[39mreturn\u001b[39;00m train_pool\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\catboost\\core.py:735\u001b[0m, in \u001b[0;36mPool.__init__\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m    734\u001b[0m     \u001b[39mif\u001b[39;00m (data\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m (cat_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mlen\u001b[39m(cat_features) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m--> 735\u001b[0m         \u001b[39mraise\u001b[39;00m CatBoostError(\n\u001b[0;32m    736\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is numpy array of floating point numerical type, it means no categorical features,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    737\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m but \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcat_features\u001b[39m\u001b[39m'\u001b[39m\u001b[39m parameter specifies nonzero number of categorical features\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    738\u001b[0m         )\n\u001b[0;32m    739\u001b[0m     \u001b[39mif\u001b[39;00m (data\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m (text_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mlen\u001b[39m(text_features) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m    740\u001b[0m         \u001b[39mraise\u001b[39;00m CatBoostError(\n\u001b[0;32m    741\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is numpy array of floating point numerical type, it means no text features,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    742\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m but \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtext_features\u001b[39m\u001b[39m'\u001b[39m\u001b[39m parameter specifies nonzero number of text features\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    743\u001b[0m         )\n",
      "\u001b[1;31mCatBoostError\u001b[0m: 'data' is numpy array of floating point numerical type, it means no categorical features, but 'cat_features' parameter specifies nonzero number of categorical features"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "#\n",
    "#  OPTIMIZED - CatBoost\n",
    "# \n",
    "########################\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "categorical_features_indices =[0,2,25,26,27]\n",
    "\n",
    "\n",
    "# Best hyperparameters found (after hyperparameter tuning)\n",
    "best_params = {'depth': 8, 'iterations': 200, 'learning_rate': 0.1}\n",
    "\n",
    "# Instantiate CatBoostRegressor with the best hyperparameters\n",
    "model_name = CatBoostRegressor(**best_params)\n",
    "\n",
    "numeric_transform = Pipeline([('impute_mean', SimpleImputer(strategy='mean')),\n",
    "                              ('scaling', StandardScaler())])\n",
    "\n",
    "categorical_transform = Pipeline([('impute_mode', SimpleImputer(strategy='most_frequent')),\n",
    "                                  ('one-hot-encode', OneHotEncoder(sparse_output=False))])\n",
    "\n",
    "preprocessing_df = ColumnTransformer([('numeric', numeric_transform, COLS_TO_SCALE),\n",
    "                                      ('categorical', categorical_transform, COLS_TO_SCALE)])\n",
    "\n",
    "\n",
    "pipeline_base = Pipeline([('proprocessing', preprocessing_df),\n",
    "                    ('model', model_name)])\n",
    "pipeline_base.fit(X_train, y_train, model__cat_features=categorical_features_indices)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = pipeline_base.predict(X_validate)\n",
    "r2 = pipeline_base.score(X_validate, y_validate)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_validate, y_pred, squared=False))\n",
    "print(f'Model employed: {model_name}')\n",
    "print(f'R-squared of base model: {r2}')\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lighthouse_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
