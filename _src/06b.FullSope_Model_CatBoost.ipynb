{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from operator import itemgetter    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data from Pre-Processing\n",
    "* In this scenario, we are using only the surgeries with the greatest volume count (id_category: 08R). \n",
    "* Missing values HAVE been imputed.\n",
    "* No PCA performed yet, no 1hot encoding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# Trial with SINGLE Operation Category\n",
    "#\n",
    "#####################\n",
    "\n",
    "df= pd.read_csv('../_data/operations_imputed_CLEAN_v2.csv', index_col=0)\n",
    "\n",
    "# Going for it - doing the whole deal\n",
    "# df = df[df['category_id']=='08R']\n",
    "df.drop(['race'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 76742 entries, 8 to 128030\n",
      "Data columns (total 37 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   op_id            76742 non-null  int64  \n",
      " 1   subject_id       76742 non-null  int64  \n",
      " 2   hadm_id          76742 non-null  int64  \n",
      " 3   opdate           76742 non-null  int64  \n",
      " 4   age              76742 non-null  int64  \n",
      " 5   sex              76742 non-null  object \n",
      " 6   weight           76742 non-null  float64\n",
      " 7   height           76742 non-null  float64\n",
      " 8   asa              76742 non-null  float64\n",
      " 9   department       76742 non-null  object \n",
      " 10  antype           76742 non-null  object \n",
      " 11  icd10_pcs        76742 non-null  object \n",
      " 12  category_desc    76742 non-null  object \n",
      " 13  desc_short       76742 non-null  object \n",
      " 14  category_id      76742 non-null  object \n",
      " 15  hr               76742 non-null  float64\n",
      " 16  pip              76742 non-null  float64\n",
      " 17  pmean            76742 non-null  float64\n",
      " 18  rr               76742 non-null  float64\n",
      " 19  spo2             76742 non-null  float64\n",
      " 20  vt               76742 non-null  float64\n",
      " 21  chloride         76742 non-null  float64\n",
      " 22  creatinine       76742 non-null  float64\n",
      " 23  glucose          76742 non-null  float64\n",
      " 24  hb               76742 non-null  float64\n",
      " 25  hco3             76742 non-null  float64\n",
      " 26  lymphocyte       76742 non-null  float64\n",
      " 27  platelet         76742 non-null  float64\n",
      " 28  potassium        76742 non-null  float64\n",
      " 29  sodium           76742 non-null  float64\n",
      " 30  total_bilirubin  76742 non-null  float64\n",
      " 31  wbc              76742 non-null  float64\n",
      " 32  LOS              76742 non-null  float64\n",
      " 33  prolonged_LOS    76742 non-null  int64  \n",
      " 34  icu_visit        76742 non-null  int64  \n",
      " 35  or_duration      76742 non-null  float64\n",
      " 36  anesth_duration  76742 non-null  float64\n",
      "dtypes: float64(23), int64(7), object(7)\n",
      "memory usage: 22.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the X and y DataFrames\n",
    "\n",
    "  * create y\n",
    "  * create X (complete with all the features)\n",
    "  * drop the features we identified as not meeting impact threshold. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 76742 entries, 8 to 128030\n",
      "Data columns (total 28 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   category_id      76742 non-null  object \n",
      " 1   age              76742 non-null  int64  \n",
      " 2   sex              76742 non-null  object \n",
      " 3   weight           76742 non-null  float64\n",
      " 4   height           76742 non-null  float64\n",
      " 5   hr               76742 non-null  float64\n",
      " 6   pip              76742 non-null  float64\n",
      " 7   pmean            76742 non-null  float64\n",
      " 8   rr               76742 non-null  float64\n",
      " 9   spo2             76742 non-null  float64\n",
      " 10  vt               76742 non-null  float64\n",
      " 11  chloride         76742 non-null  float64\n",
      " 12  creatinine       76742 non-null  float64\n",
      " 13  glucose          76742 non-null  float64\n",
      " 14  hb               76742 non-null  float64\n",
      " 15  hco3             76742 non-null  float64\n",
      " 16  lymphocyte       76742 non-null  float64\n",
      " 17  platelet         76742 non-null  float64\n",
      " 18  potassium        76742 non-null  float64\n",
      " 19  sodium           76742 non-null  float64\n",
      " 20  total_bilirubin  76742 non-null  float64\n",
      " 21  wbc              76742 non-null  float64\n",
      " 22  icu_visit        76742 non-null  int64  \n",
      " 23  or_duration      76742 non-null  float64\n",
      " 24  anesth_duration  76742 non-null  float64\n",
      " 25  asa              76742 non-null  float64\n",
      " 26  department       76742 non-null  object \n",
      " 27  antype           76742 non-null  object \n",
      "dtypes: float64(22), int64(2), object(4)\n",
      "memory usage: 17.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Label = LOS\n",
    "\n",
    "# When doing a Categorical Model, reinsert 'prolonged_LOS' and instead, drop 'LOS'\n",
    "\n",
    "features_to_retain = ['category_id','age','sex',\t'weight',\t'height',\t'hr',\t'pip',\t'pmean',\t'rr',\t'spo2',\t'vt',\t'chloride',\t'creatinine',\t'glucose',\t'hb',\t'hco3',\t'lymphocyte',\t'platelet',\t'potassium',\t'sodium',\t'total_bilirubin',\t'wbc',\t'icu_visit',\t'or_duration',\t'anesth_duration',\t'asa','department','antype'] \n",
    "\n",
    "\n",
    "y = df['LOS']\n",
    "X = df.drop('LOS', axis=1)\n",
    "# Get a list of column names with data type 'object'\n",
    "\n",
    "\n",
    "X= X[features_to_retain]\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'weight', 'height', 'hr', 'pip', 'pmean', 'rr', 'spo2', 'vt',\n",
      "       'chloride', 'creatinine', 'glucose', 'hb', 'hco3', 'lymphocyte',\n",
      "       'platelet', 'potassium', 'sodium', 'total_bilirubin', 'wbc',\n",
      "       'icu_visit', 'or_duration', 'anesth_duration'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 76742 entries, 8 to 128030\n",
      "Data columns (total 28 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   category_id      76742 non-null  object \n",
      " 1   age              76742 non-null  int64  \n",
      " 2   sex              76742 non-null  object \n",
      " 3   weight           76742 non-null  float64\n",
      " 4   height           76742 non-null  float64\n",
      " 5   hr               76742 non-null  float64\n",
      " 6   pip              76742 non-null  float64\n",
      " 7   pmean            76742 non-null  float64\n",
      " 8   rr               76742 non-null  float64\n",
      " 9   spo2             76742 non-null  float64\n",
      " 10  vt               76742 non-null  float64\n",
      " 11  chloride         76742 non-null  float64\n",
      " 12  creatinine       76742 non-null  float64\n",
      " 13  glucose          76742 non-null  float64\n",
      " 14  hb               76742 non-null  float64\n",
      " 15  hco3             76742 non-null  float64\n",
      " 16  lymphocyte       76742 non-null  float64\n",
      " 17  platelet         76742 non-null  float64\n",
      " 18  potassium        76742 non-null  float64\n",
      " 19  sodium           76742 non-null  float64\n",
      " 20  total_bilirubin  76742 non-null  float64\n",
      " 21  wbc              76742 non-null  float64\n",
      " 22  icu_visit        76742 non-null  int64  \n",
      " 23  or_duration      76742 non-null  float64\n",
      " 24  anesth_duration  76742 non-null  float64\n",
      " 25  asa              76742 non-null  object \n",
      " 26  department       76742 non-null  object \n",
      " 27  antype           76742 non-null  object \n",
      "dtypes: float64(21), int64(2), object(5)\n",
      "memory usage: 17.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Indentify the columns that need to be either cast as Str or Scaled\n",
    "############################################################\n",
    "\n",
    "COLS_TO_CAST = ['asa','sex','department','antype'] #When restoring scope to full category list, add cat_id here.\n",
    "# Convert the object data type columns to string\n",
    "\n",
    "X[COLS_TO_CAST] = X[COLS_TO_CAST].astype(str)\n",
    "\n",
    "# Filter columns with dtype 'numeric' for scaling later in the Pipleine\n",
    "COLS_TO_SCALE = X.select_dtypes(include=['int', 'float']).columns\n",
    "\n",
    "print(COLS_TO_SCALE)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data\n",
    "- Training Set (80% of total): \n",
    "  - Used to train the models.\n",
    "- Validation Set (20% of Traning Set ): \n",
    "  - Used to fine-tune hyperparameters, select models, and monitor training progress.  \n",
    "- Testing Set (20% of total): \n",
    "  - Used to evaluate the final model's performance on unseen data and estimate its generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (49114, 28)\n",
      "X_validate shape: (12279, 28)\n",
      "X_test shape: (15349, 28)\n",
      "y_train shape: (49114,)\n",
      "y_validate shape: (12279,)\n",
      "y_test shape: (15349,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SPLIT = .2\n",
    "TRAINING_SPLIT = 1-TEST_SPLIT\n",
    "VALIDATION_SPLIT = .2\n",
    "\n",
    "# Split data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=85100)\n",
    "\n",
    "# Split the Training AGAIN into train and Validate\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=TEST_SPLIT, random_state=85100)\n",
    "\n",
    "# Then, you can use X_train and y_train for model training and X_test and y_test for evaluation.\n",
    "\n",
    "data_subset_dict = {\n",
    "    'X_train': X_train,\n",
    "    'X_validate': X_validate,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_validate': y_validate,\n",
    "    'y_test': y_test}\n",
    "\n",
    "for key, value in data_subset_dict.items():\n",
    "    shape = value.shape\n",
    "    print(f\"{key} shape: {shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "#  SIMPLE LINEAR REGRESSION Pipeline -\n",
    "#  -- No tuning. \n",
    "########################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "numeric_transform = Pipeline([('impute_mean', SimpleImputer(strategy='mean')),\n",
    "                              ('scaling', StandardScaler())])\n",
    "\n",
    "categorical_transform = Pipeline([('impute_mode', SimpleImputer(strategy='most_frequent')),\n",
    "                                  ('one-hot-encode', OneHotEncoder(sparse_output=False))])\n",
    "\n",
    "preprocessing_df = ColumnTransformer([('numeric', numeric_transform, ['age','weight',\t'height',\t'art_mbp',\t'art_sbp',\t'bt',\t'cvp',\t'hr',\t'pip',\t'pmean',\t'rr',\t'spo2',\t'vt',\t'alp',\t'alt',\t'ast',\t'chloride',\t'creatinine',\t'glucose',\t'hb',\t'hco3',\t'lymphocyte',\t'platelet',\t'potassium',\t'sodium',\t'total_bilirubin',\t'wbc',\t'is_outlier',\t'prolonged_LOS',\t'icu_visit',\t'or_duration',\t'anesth_duration']),\n",
    "                                      ('categorical', categorical_transform, ['sex','asa','department','antype'])])\n",
    "\n",
    "\n",
    "pipeline_base = Pipeline([('proprocessing', preprocessing_df),\n",
    "                    ('model', LinearRegression())])\n",
    "pipeline_base.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = pipeline_base.predict(X_validate)\n",
    "r2 = pipeline_base.score(X_validate, y_validate)\n",
    "\n",
    "mse = mean_squared_error(y_validate, y_pred, squared=False)\n",
    "rmse = sqrt(mse)\n",
    "print(f'R-squared of base model: {r2}')\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "#  Ensemble Pipeline -\n",
    "#  -- No tuning. Change the variable \"model_name\" for other models. \n",
    "########################\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Model with Hyperparameter Tuning via Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "#  STANDALONE TUNING  - CatBoost\n",
    "# \n",
    "########################\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from catboost import Pool, CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def rmse_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    rmse = sqrt(mse)\n",
    "    return -rmse  # Return negative RMSE for grid search to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "categorical_features_indices =[1,33,34,35]\n",
    "\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'iterations': [100, 200, 300],      # Number of boosting iterations\n",
    "    'depth': [6, 8, 10],                # Depth of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate  \n",
    "    }\n",
    "\n",
    "# Create a CatBoostRegressor model\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=catboost_model, param_grid=param_grid, cv=5, scoring=rmse_scorer, n_jobs=-1, error_score='raise')\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train, y_train, cat_features=categorical_features_indices)\n",
    "\n",
    "# Print the best hyperparameters and corresponding MSE score\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Best RMSE score:\", -grid_search.best_score_)\n",
    "\n",
    "# Get the best trained model\n",
    "best_catboost_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "validation_predictions = best_catboost_model.predict(X_validate)\n",
    "\n",
    "# Output - best settings for training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "#  OPTIMIZED - CatBoost\n",
    "# \n",
    "########################\n",
    "categorical_features_indices =[0,2,26,27]\n",
    "\n",
    "\n",
    "# Best hyperparameters found (after hyperparameter tuning)\n",
    "best_params = {'depth': 8, 'iterations': 200, 'learning_rate': 0.1}\n",
    "\n",
    "# Instantiate CatBoostRegressor with the best hyperparameters\n",
    "model_name = CatBoostRegressor(**best_params)\n",
    "\n",
    "numeric_transform = Pipeline([('impute_mean', SimpleImputer(strategy='mean')),\n",
    "                              ('scaling', StandardScaler())])\n",
    "\n",
    "categorical_transform = Pipeline([('impute_mode', SimpleImputer(strategy='most_frequent')),\n",
    "                                  ('one-hot-encode', OneHotEncoder(sparse_output=False))])\n",
    "\n",
    "preprocessing_df = ColumnTransformer([('numeric', numeric_transform, COLS_TO_SCALE),\n",
    "                                      ('categorical', categorical_transform, COLS_TO_SCALE)])\n",
    "\n",
    "\n",
    "pipeline_base = Pipeline([('proprocessing', preprocessing_df),\n",
    "                    ('model', model_name)])\n",
    "pipeline_base.fit(X_train, y_train, model__cat_features=categorical_features_indices)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = pipeline_base.predict(X_validate)\n",
    "r2 = pipeline_base.score(X_validate, y_validate)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_validate, y_pred, squared=False))\n",
    "print(f'Model employed: {model_name}')\n",
    "print(f'R-squared of base model: {r2}')\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lighthouse_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
