{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: This is Python 3 code.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt # NOTE: This was tested with matplotlib v. 2.1.0\n",
    " \n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 91862 entries, 0 to 91861\n",
      "Data columns (total 63 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Unnamed: 0         91862 non-null  int64  \n",
      " 1   op_id              91862 non-null  int64  \n",
      " 2   subject_id         91862 non-null  int64  \n",
      " 3   hadm_id            91862 non-null  int64  \n",
      " 4   opdate             91862 non-null  int64  \n",
      " 5   age                91862 non-null  int64  \n",
      " 6   sex                91862 non-null  object \n",
      " 7   weight             90910 non-null  float64\n",
      " 8   height             91326 non-null  float64\n",
      " 9   race               91862 non-null  object \n",
      " 10  asa                89532 non-null  float64\n",
      " 11  emop               91862 non-null  int64  \n",
      " 12  department         91862 non-null  object \n",
      " 13  antype             91862 non-null  object \n",
      " 14  icd10_pcs          91862 non-null  object \n",
      " 15  category_desc      91862 non-null  object \n",
      " 16  desc_short         91862 non-null  object \n",
      " 17  category_id        91862 non-null  object \n",
      " 18  orin_time          91862 non-null  int64  \n",
      " 19  orout_time         91862 non-null  int64  \n",
      " 20  opstart_time       91854 non-null  float64\n",
      " 21  opend_time         91854 non-null  float64\n",
      " 22  admission_time     91862 non-null  int64  \n",
      " 23  discharge_time     91862 non-null  int64  \n",
      " 24  anstart_time       91823 non-null  float64\n",
      " 25  anend_time         91612 non-null  float64\n",
      " 26  cpbon_time         2153 non-null   float64\n",
      " 27  cpboff_time        2153 non-null   float64\n",
      " 28  icuin_time         11210 non-null  float64\n",
      " 29  icuout_time        11210 non-null  float64\n",
      " 30  inhosp_death_time  969 non-null    float64\n",
      " 31  subject_id_y       91836 non-null  float64\n",
      " 32  chart_time_x       91836 non-null  float64\n",
      " 33  art_dbp            26232 non-null  float64\n",
      " 34  art_mbp            89504 non-null  float64\n",
      " 35  art_sbp            89497 non-null  float64\n",
      " 36  bt                 89479 non-null  float64\n",
      " 37  cvp                89248 non-null  float64\n",
      " 38  hr                 89515 non-null  float64\n",
      " 39  pip                89519 non-null  float64\n",
      " 40  pmean              89508 non-null  float64\n",
      " 41  rr                 89517 non-null  float64\n",
      " 42  spo2               89517 non-null  float64\n",
      " 43  vt                 89512 non-null  float64\n",
      " 44  chart_time_y       89532 non-null  float64\n",
      " 45  alp                89455 non-null  float64\n",
      " 46  alt                89465 non-null  float64\n",
      " 47  ast                89465 non-null  float64\n",
      " 48  chloride           89471 non-null  float64\n",
      " 49  creatinine         89467 non-null  float64\n",
      " 50  glucose            89527 non-null  float64\n",
      " 51  hb                 89488 non-null  float64\n",
      " 52  hco3               89520 non-null  float64\n",
      " 53  lymphocyte         89460 non-null  float64\n",
      " 54  platelet           89461 non-null  float64\n",
      " 55  potassium          89523 non-null  float64\n",
      " 56  sodium             89522 non-null  float64\n",
      " 57  total_bilirubin    89454 non-null  float64\n",
      " 58  wbc                89465 non-null  float64\n",
      " 59  LOS                89532 non-null  float64\n",
      " 60  is_outlier         91862 non-null  int64  \n",
      " 61  prolonged_LOS      91862 non-null  int64  \n",
      " 62  icu_visit          91862 non-null  bool   \n",
      "dtypes: bool(1), float64(41), int64(13), object(8)\n",
      "memory usage: 43.5+ MB\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "#\n",
    "# Data Generation Code\n",
    "#\n",
    "#########################\n",
    "df = pd.read_csv('../_data/operations_imputed_CLEAN.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'F'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jamie\\OneDrive - Lepard & Lepard\\Data Science\\LighthouseLabs\\Python_Projects\\Prolonged_LOS_Project\\_src\\05.PCA.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamie/OneDrive%20-%20Lepard%20%26%20Lepard/Data%20Science/LighthouseLabs/Python_Projects/Prolonged_LOS_Project/_src/05.PCA.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m X \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop(cols_to_drop,axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamie/OneDrive%20-%20Lepard%20%26%20Lepard/Data%20Science/LighthouseLabs/Python_Projects/Prolonged_LOS_Project/_src/05.PCA.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m y \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mLOS\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jamie/OneDrive%20-%20Lepard%20%26%20Lepard/Data%20Science/LighthouseLabs/Python_Projects/Prolonged_LOS_Project/_src/05.PCA.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m X \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit_transform(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamie/OneDrive%20-%20Lepard%20%26%20Lepard/Data%20Science/LighthouseLabs/Python_Projects/Prolonged_LOS_Project/_src/05.PCA.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m dfx \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data\u001b[39m=\u001b[39mX,columns\u001b[39m=\u001b[39mdf_final\u001b[39m.\u001b[39mcolumns[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamie/OneDrive%20-%20Lepard%20%26%20Lepard/Data%20Science/LighthouseLabs/Python_Projects/Prolonged_LOS_Project/_src/05.PCA.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m dfx\u001b[39m.\u001b[39mhead(\u001b[39m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\sklearn\\base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 915\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    918\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:837\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    836\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 837\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:873\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \n\u001b[0;32m    843\u001b[0m \u001b[39mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[39m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    872\u001b[0m first_call \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 873\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    874\u001b[0m     X,\n\u001b[0;32m    875\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    876\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    877\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    878\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_call,\n\u001b[0;32m    879\u001b[0m )\n\u001b[0;32m    880\u001b[0m n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m    882\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\sklearn\\utils\\validation.py:838\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    833\u001b[0m \u001b[39mif\u001b[39;00m pandas_requires_conversion:\n\u001b[0;32m    834\u001b[0m     \u001b[39m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[0;32m    835\u001b[0m     \u001b[39m# nans\u001b[39;00m\n\u001b[0;32m    836\u001b[0m     \u001b[39m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[0;32m    837\u001b[0m     new_dtype \u001b[39m=\u001b[39m dtype_orig \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m dtype\n\u001b[1;32m--> 838\u001b[0m     array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39;49mastype(new_dtype)\n\u001b[0;32m    839\u001b[0m     \u001b[39m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[0;32m    840\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\pandas\\core\\generic.py:6534\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6530\u001b[0m     results \u001b[39m=\u001b[39m [ser\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy) \u001b[39mfor\u001b[39;00m _, ser \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()]\n\u001b[0;32m   6532\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6533\u001b[0m     \u001b[39m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6534\u001b[0m     new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mastype(dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   6535\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[39m=\u001b[39mnew_data\u001b[39m.\u001b[39maxes)\n\u001b[0;32m   6536\u001b[0m     \u001b[39mreturn\u001b[39;00m res\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\pandas\\core\\internals\\managers.py:414\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[39melif\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    412\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\n\u001b[0;32m    415\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mastype\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    416\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    417\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    418\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    419\u001b[0m     using_cow\u001b[39m=\u001b[39;49musing_copy_on_write(),\n\u001b[0;32m    420\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\pandas\\core\\internals\\managers.py:354\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 354\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(b, f)(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    355\u001b[0m     result_blocks \u001b[39m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    357\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mfrom_blocks(result_blocks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:616\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[39mCoerce to the new dtype.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[39mBlock\u001b[39;00m\n\u001b[0;32m    613\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    614\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n\u001b[1;32m--> 616\u001b[0m new_values \u001b[39m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m    618\u001b[0m new_values \u001b[39m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    620\u001b[0m refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:238\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    235\u001b[0m     dtype \u001b[39m=\u001b[39m dtype\u001b[39m.\u001b[39mnumpy_dtype\n\u001b[0;32m    237\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 238\u001b[0m     new_values \u001b[39m=\u001b[39m astype_array(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    239\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[0;32m    240\u001b[0m     \u001b[39m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[39m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:183\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    180\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    182\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 183\u001b[0m     values \u001b[39m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    185\u001b[0m \u001b[39m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, np\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:134\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m    132\u001b[0m \u001b[39mif\u001b[39;00m copy \u001b[39mor\u001b[39;00m arr\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m \u001b[39mor\u001b[39;00m dtype \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m:\n\u001b[0;32m    133\u001b[0m     \u001b[39m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39;49mastype(dtype, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    136\u001b[0m \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'F'"
     ]
    }
   ],
   "source": [
    "\n",
    "#########################\n",
    "#\n",
    "# Perform PCA on the data\n",
    "#\n",
    "#########################\n",
    "# First center and scale the data\n",
    "\n",
    "cols_to_drop = ['op_id','subject_id','hadm_id','opdate','LOS','is_outlier','prolonged_LOS']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = df.drop(cols_to_drop,axis=1)\n",
    "y = df['LOS']\n",
    "X = scaler.fit_transform(X)\n",
    "dfx = pd.DataFrame(data=X,columns=df_final.columns[:-2])\n",
    "dfx.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaled_data = preprocessing.scale(df.T)\n",
    " \n",
    "pca = PCA() # create a PCA object\n",
    "pca.fit(scaled_data) # do the math\n",
    "pca_data = pca.transform(scaled_data) # get PCA coordinates for scaled_data\n",
    " \n",
    "#########################\n",
    "#\n",
    "# Draw a scree plot and a PCA plot\n",
    "#\n",
    "#########################\n",
    " \n",
    "#The following code constructs the Scree plot\n",
    "per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    " \n",
    "plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()\n",
    " \n",
    "#the following code makes a fancy looking plot using PC1 and PC2\n",
    "pca_df = pd.DataFrame(pca_data, index=[*wt, *ko], columns=labels)\n",
    " \n",
    "plt.scatter(pca_df.PC1, pca_df.PC2)\n",
    "plt.title('My PCA Graph')\n",
    "plt.xlabel('PC1 - {0}%'.format(per_var[0]))\n",
    "plt.ylabel('PC2 - {0}%'.format(per_var[1]))\n",
    " \n",
    "for sample in pca_df.index:\n",
    "    plt.annotate(sample, (pca_df.PC1.loc[sample], pca_df.PC2.loc[sample]))\n",
    " \n",
    "plt.show()\n",
    " \n",
    "#########################\n",
    "#\n",
    "# Determine which genes had the biggest influence on PC1\n",
    "#\n",
    "#########################\n",
    " \n",
    "## get the name of the top 10 measurements (genes) that contribute\n",
    "## most to pc1.\n",
    "## first, get the loading scores\n",
    "loading_scores = pd.Series(pca.components_[0], index=genes)\n",
    "## now sort the loading scores based on their magnitude\n",
    "sorted_loading_scores = loading_scores.abs().sort_values(ascending=False)\n",
    " \n",
    "# get the names of the top 10 genes\n",
    "top_10_genes = sorted_loading_scores[0:10].index.values\n",
    " \n",
    "## print the gene names and their scores (and +/- sign)\n",
    "print(loading_scores[top_10_genes])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lighthouse_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
