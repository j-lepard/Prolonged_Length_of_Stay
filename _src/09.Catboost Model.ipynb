{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92e48866",
   "metadata": {},
   "source": [
    "##  Catboost Model\n",
    "### Pipeline setup, Train, Pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25791a74",
   "metadata": {},
   "source": [
    "#### 1.1 Import Data and Required Packages\n",
    "##### Importing Pandas, Numpy, Matplotlib, Seaborn and Warings Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b080dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "# Modelling\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, Ridge,Lasso\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer,OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45079ad",
   "metadata": {},
   "source": [
    "#### Import the CSV Data as Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e11c6255",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# Import Data from PreProcessing\n",
    "#\n",
    "#####################\n",
    "\n",
    "df= pd.read_csv('../../Prolonged_LOS_Project/_data/operations_imputed_CLEAN_v2.csv', index_col=0)\n",
    "\n",
    "df.drop(['race'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20634923",
   "metadata": {},
   "source": [
    "#### Show Top 5 Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e412a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>op_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>opdate</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>asa</th>\n",
       "      <th>department</th>\n",
       "      <th>...</th>\n",
       "      <th>platelet</th>\n",
       "      <th>potassium</th>\n",
       "      <th>sodium</th>\n",
       "      <th>total_bilirubin</th>\n",
       "      <th>wbc</th>\n",
       "      <th>LOS</th>\n",
       "      <th>prolonged_LOS</th>\n",
       "      <th>icu_visit</th>\n",
       "      <th>or_duration</th>\n",
       "      <th>anesth_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>467425045</td>\n",
       "      <td>134213281</td>\n",
       "      <td>225860669</td>\n",
       "      <td>1440</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>62.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GS</td>\n",
       "      <td>...</td>\n",
       "      <td>217.282759</td>\n",
       "      <td>3.846584</td>\n",
       "      <td>140.033084</td>\n",
       "      <td>0.744921</td>\n",
       "      <td>8.200501</td>\n",
       "      <td>3.493056</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>461473883</td>\n",
       "      <td>134195201</td>\n",
       "      <td>265770645</td>\n",
       "      <td>1440</td>\n",
       "      <td>35</td>\n",
       "      <td>F</td>\n",
       "      <td>50.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>OS</td>\n",
       "      <td>...</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>6.310000</td>\n",
       "      <td>4.236111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>430539801</td>\n",
       "      <td>181420324</td>\n",
       "      <td>208290342</td>\n",
       "      <td>1440</td>\n",
       "      <td>20</td>\n",
       "      <td>M</td>\n",
       "      <td>62.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>OL</td>\n",
       "      <td>...</td>\n",
       "      <td>237.222222</td>\n",
       "      <td>4.041013</td>\n",
       "      <td>139.824013</td>\n",
       "      <td>1.078906</td>\n",
       "      <td>10.044925</td>\n",
       "      <td>1.572917</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>466389608</td>\n",
       "      <td>160947402</td>\n",
       "      <td>262240911</td>\n",
       "      <td>1440</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>52.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>OL</td>\n",
       "      <td>...</td>\n",
       "      <td>217.282759</td>\n",
       "      <td>3.846584</td>\n",
       "      <td>140.033084</td>\n",
       "      <td>0.744921</td>\n",
       "      <td>8.200501</td>\n",
       "      <td>1.607639</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>439560439</td>\n",
       "      <td>163619571</td>\n",
       "      <td>279388936</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>F</td>\n",
       "      <td>65.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>OT</td>\n",
       "      <td>...</td>\n",
       "      <td>207.270270</td>\n",
       "      <td>3.867044</td>\n",
       "      <td>139.499091</td>\n",
       "      <td>0.739452</td>\n",
       "      <td>8.552474</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        op_id  subject_id    hadm_id  opdate  age sex  weight  height  asa  \\\n",
       "8   467425045   134213281  225860669    1440   60   F    62.0   154.0  1.0   \n",
       "9   461473883   134195201  265770645    1440   35   F    50.0   160.0  1.0   \n",
       "10  430539801   181420324  208290342    1440   20   M    62.0   179.0  1.0   \n",
       "11  466389608   160947402  262240911    1440   60   F    52.0   152.0  1.0   \n",
       "15  439560439   163619571  279388936       0   75   F    65.0   154.0  2.0   \n",
       "\n",
       "   department  ...    platelet potassium      sodium total_bilirubin  \\\n",
       "8          GS  ...  217.282759  3.846584  140.033084        0.744921   \n",
       "9          OS  ...  124.000000  3.900000  138.000000        0.600000   \n",
       "10         OL  ...  237.222222  4.041013  139.824013        1.078906   \n",
       "11         OL  ...  217.282759  3.846584  140.033084        0.744921   \n",
       "15         OT  ...  207.270270  3.867044  139.499091        0.739452   \n",
       "\n",
       "          wbc       LOS  prolonged_LOS  icu_visit  or_duration  \\\n",
       "8    8.200501  3.493056              1          0         70.0   \n",
       "9    6.310000  4.236111              0          0        115.0   \n",
       "10  10.044925  1.572917              1          0         90.0   \n",
       "11   8.200501  1.607639              0          0         30.0   \n",
       "15   8.552474  0.604167              0          0         15.0   \n",
       "\n",
       "    anesth_duration  \n",
       "8              90.0  \n",
       "9             150.0  \n",
       "10            135.0  \n",
       "11             90.0  \n",
       "15             25.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd32281",
   "metadata": {},
   "source": [
    "#### Preparing X and Y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d72fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 76742 entries, 8 to 128030\n",
      "Data columns (total 27 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   category_id      76742 non-null  object \n",
      " 1   age              76742 non-null  int64  \n",
      " 2   sex              76742 non-null  object \n",
      " 3   weight           76742 non-null  float64\n",
      " 4   height           76742 non-null  float64\n",
      " 5   hr               76742 non-null  float64\n",
      " 6   pip              76742 non-null  float64\n",
      " 7   pmean            76742 non-null  float64\n",
      " 8   rr               76742 non-null  float64\n",
      " 9   spo2             76742 non-null  float64\n",
      " 10  vt               76742 non-null  float64\n",
      " 11  chloride         76742 non-null  float64\n",
      " 12  creatinine       76742 non-null  float64\n",
      " 13  glucose          76742 non-null  float64\n",
      " 14  hb               76742 non-null  float64\n",
      " 15  hco3             76742 non-null  float64\n",
      " 16  lymphocyte       76742 non-null  float64\n",
      " 17  platelet         76742 non-null  float64\n",
      " 18  potassium        76742 non-null  float64\n",
      " 19  sodium           76742 non-null  float64\n",
      " 20  total_bilirubin  76742 non-null  float64\n",
      " 21  wbc              76742 non-null  float64\n",
      " 22  icu_visit        76742 non-null  int64  \n",
      " 23  or_duration      76742 non-null  float64\n",
      " 24  anesth_duration  76742 non-null  float64\n",
      " 25  department       76742 non-null  object \n",
      " 26  antype           76742 non-null  object \n",
      "dtypes: float64(21), int64(2), object(4)\n",
      "memory usage: 16.4+ MB\n"
     ]
    }
   ],
   "source": [
    "## Features to retain are those in X that will be used in training. Exludued features are features such as Operation_ID, Subject_ID..\n",
    "features_to_retain = ['category_id','age','sex',\t'weight',\t'height',\t'hr',\t'pip',\t'pmean',\t'rr',\t'spo2',\t'vt',\t'chloride',\t'creatinine',\t'glucose',\t'hb',\t'hco3',\t'lymphocyte',\t'platelet',\t'potassium',\t'sodium',\t'total_bilirubin',\t'wbc',\t'icu_visit',\t'or_duration',\t'anesth_duration',\t'department','antype'] \n",
    "\n",
    "## Create the Y, the Target - Round LOS to an integer because we dont want to predict the precison of LOS to a decimal... \n",
    "# that means they would be discharging at strange times like 2:45am (not likely)\n",
    "y = df['LOS'].round().astype(int)\n",
    "\n",
    "\n",
    "## Create X the Features for Train/Test/Validate\n",
    "X = df.drop('LOS', axis=1)\n",
    "X= X[features_to_retain]\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd613177",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc69816",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cofirm what Y looks like. (should be straing integers)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcessor Pipeline (X - features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e290fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Column Transformer with 3 types of transformers\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "COLS_TO_CAST = ['category_id','antype','sex','department', 'icu_visit']\n",
    "\n",
    "# Define the columns that you want to scale and cast to strings\n",
    "numeric_features = X.select_dtypes(include=['int', 'float']).columns\n",
    "string_features =  COLS_TO_CAST\n",
    "\n",
    "# Define transformers\n",
    "numeric_transformer = Pipeline([('impute_mean', SimpleImputer(strategy='mean')),\n",
    "                              ('scaling', StandardScaler())])\n",
    "string_transformer = FunctionTransformer(lambda x: x.astype(str), validate=False)\n",
    "categorical_transform = Pipeline([('impute_mode', SimpleImputer(strategy='most_frequent'))])\n",
    "# onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Combine transformers into a preprocessor with ColumnTransformer\n",
    "preprocessor_catboost = ColumnTransformer(\n",
    "                        transformers=[\n",
    "                                    ('num', numeric_transformer, numeric_features),\n",
    "                                    ('categ', categorical_transform,string_features),\n",
    "                                    ('str', string_transformer, string_features)],\n",
    "                        remainder='passthrough')  # Leaves the rest of the columns alone)\n",
    "\n",
    "'''\n",
    "NOTE 1 - OneHot Encoding has be REMOVED from the preproccing pipeline here because it is handled within the `catboost` model. \n",
    "\n",
    "NOTE 2 - If doing a single model in the pipeline, you could use the below snippet. In this case, we want to separate preprocessing pipeline so it can be used again in production (ie predicting using user-provided data)\n",
    "\n",
    "# Create a full pipeline by combining with an estimator, for example, a classifier\n",
    "# pipeline_LR = Pipeline(steps=[\n",
    "    # ('preprocessor', preprocessor),\n",
    "    # ('model', LinearRegression())])\n",
    "\n",
    "TODO - If there is problem with preprocess on incomplete data - check that ctegory transformer isnt the reason\n",
    "'''\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Package (pickle the preproccessor)\n",
    "# Preprocessor with be referenced again in production to scale/transform user-provided data.\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('../output/preprocessor.pickle', 'wb') as f:\n",
    "    pickle.dump(preprocessor_catboost, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load from a file using Pickle\n",
    "try:\n",
    "    with open('../output/preprocessor.pickle', 'rb') as f:\n",
    "        preprocessor_catboost = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"File 'preprocessor_catboost' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72459f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessor_catboost.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c4e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate dataset into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SPLIT = .2\n",
    "TRAINING_SPLIT = 1-TEST_SPLIT\n",
    "VALIDATION_SPLIT = .2\n",
    "\n",
    "\n",
    "def split_data(X, y, test_split=0.2, validation_split=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Splits data into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Features data.\n",
    "    - y: Target variable.\n",
    "    - test_split: Fraction of the data to be used as test set.\n",
    "    - validation_split: Fraction of the training data to be used as validation set.\n",
    "    - random_state: Seed for the random number generator.\n",
    "\n",
    "    Returns:\n",
    "    X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_split, random_state=random_state)\n",
    "\n",
    "    # Adjust validation split to account for the initial test split\n",
    "    validation_size = validation_split / (1 - test_split)\n",
    "\n",
    "    # Split the training data again into training and validation sets\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(\n",
    "        X_train, y_train, test_size=validation_size, random_state=random_state)\n",
    "\n",
    "    # Create a dictionary to hold the data splits\n",
    "    data_splits = {\n",
    "        'X_train': X_train,\n",
    "        'X_validate': X_validate,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_validate': y_validate,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "    #Print the shapes of the splits\n",
    "    for key, value in data_splits.items():\n",
    "        shape = value.shape\n",
    "        print(f\"{key} shape: {shape}\")\n",
    "        \n",
    "    return X_train, X_validate, X_test, y_train, y_validate, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call data split function:\n",
    "X_train, X_validate, X_test, y_train, y_validate, y_test = split_data(X, y,test_split=TEST_SPLIT,\n",
    "    validation_split=VALIDATION_SPLIT,random_state=85100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evalution and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd80317",
   "metadata": {},
   "source": [
    "#### Create an Evaluate Function to give all metrics after model Training\n",
    "`Note`: This has been run previously and determined Catboost was most applicable for this problem. Skip to Model Hypertuning below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c247bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(true, predicted):\n",
    "    mae = mean_absolute_error(true, predicted)\n",
    "    mse = mean_squared_error(true, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(true, predicted))\n",
    "    r2_square = r2_score(true, predicted)\n",
    "    return mae, rmse, r2_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ccb8e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "#\n",
    "# Baseline Model Training (various models)\n",
    "# Comment-out models to be excluded\n",
    "#\n",
    "#####################\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"K-Neighbors Regressor\": KNeighborsRegressor(n_jobs=-1),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(n_jobs=-1),\n",
    "    \"XGBRegressor\": XGBRegressor(n_jobs=-1), \n",
    "    \"CatBoosting Regressor\": CatBoostRegressor(verbose=False),\n",
    "    \"AdaBoost Regressor\": AdaBoostRegressor()\n",
    "}\n",
    "model_list = []\n",
    "r2_list =[]\n",
    "\n",
    "for i in range(len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    model.fit(X_train, y_train) # Train model\n",
    "\n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate Train and Test dataset\n",
    "    model_train_mae , model_train_rmse, model_train_r2 = evaluate_model(y_train, y_train_pred)\n",
    "\n",
    "    model_test_mae , model_test_rmse, model_test_r2 = evaluate_model(y_test, y_test_pred)\n",
    "\n",
    "    \n",
    "    print(list(models.keys())[i])\n",
    "    model_list.append(list(models.keys())[i])\n",
    "    \n",
    "    print('Model performance for Training set')\n",
    "    print(\"- Root Mean Squared Error: {:.4f}\".format(model_train_rmse))\n",
    "    print(\"- Mean Absolute Error: {:.4f}\".format(model_train_mae))\n",
    "    print(\"- R2 Score: {:.4f}\".format(model_train_r2))\n",
    "\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    print('Model performance for Test set')\n",
    "    print(\"- Root Mean Squared Error: {:.4f}\".format(model_test_rmse))\n",
    "    print(\"- Mean Absolute Error: {:.4f}\".format(model_test_mae))\n",
    "    print(\"- R2 Score: {:.4f}\".format(model_test_r2))\n",
    "    r2_list.append(model_test_r2)\n",
    "    \n",
    "    print('='*35)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06480b5a",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0159e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(zip(model_list, r2_list)), columns=['Model Name', 'R2_Score']).sort_values(by=[\"R2_Score\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost Parameter Tuning\n",
    "\n",
    "* Catboost does NOT require 1-hot encoding in preprocessing. \n",
    "* Instead categorical features must be declared by index.\n",
    "* PCA was not used in the problem as the large number of elements with the 'category_id' would lead to problems with overfit and/or interpretation. \n",
    "* tSNE will be considered at a later time point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "# Declare the categorical features that would be 1-hot encoded (not all the categorical cols)\n",
    "# This only needs to be done during training model (ie not required as part of preprocessing)\n",
    "categorical_features_indices =[0, 2, 22,25, 26]\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'iterations': [100, 200, 300],      # Number of boosting iterations\n",
    "    'depth': [6, 8, 10],                # Depth of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate  \n",
    "    }\n",
    "\n",
    "# Create a CatBoostRegressor model\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search_lin = GridSearchCV(estimator=catboost_model, param_grid=param_grid, cv=5, scoring=evaluate_model, n_jobs=-1, error_score='raise')\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search_lin.fit(X_train, y_train, cat_features=categorical_features_indices)\n",
    "\n",
    "# Print the best hyperparameters and corresponding MSE score\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(grid_search_lin.best_params_)\n",
    "print(\"Best RMSE score:\", -grid_search_lin.best_score_)\n",
    "\n",
    "# Get the best trained model\n",
    "best_catboost_model_lin = grid_search_lin.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "validation_predictions = best_catboost_model_lin.predict(X_validate)\n",
    "\n",
    "########################\n",
    "## Output:\n",
    "########################\n",
    "'''Best hyperparameters found:\n",
    "{'depth': 10, 'iterations': 300, 'learning_rate': 0.1}\n",
    "Best RMSE score: 3.834154628241867'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
