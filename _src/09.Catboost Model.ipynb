{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92e48866",
   "metadata": {},
   "source": [
    "##  Catboost Model\n",
    "### Pipeline setup, Train, Pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25791a74",
   "metadata": {},
   "source": [
    "#### 1.1 Import Data and Required Packages\n",
    "##### Importing Pandas, Numpy, Matplotlib, Seaborn and Warings Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b080dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "# Modelling\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, Ridge,Lasso\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer,OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45079ad",
   "metadata": {},
   "source": [
    "#### Import the CSV Data as Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e11c6255",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# Import Data from PreProcessing\n",
    "#\n",
    "#####################\n",
    "\n",
    "df= pd.read_csv('../../Prolonged_LOS_Project/_data/operations_imputed_CLEAN_v2.csv', index_col=0)\n",
    "\n",
    "df.drop(['race'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20634923",
   "metadata": {},
   "source": [
    "#### Show Top 5 Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e412a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>op_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>opdate</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>asa</th>\n",
       "      <th>department</th>\n",
       "      <th>...</th>\n",
       "      <th>platelet</th>\n",
       "      <th>potassium</th>\n",
       "      <th>sodium</th>\n",
       "      <th>total_bilirubin</th>\n",
       "      <th>wbc</th>\n",
       "      <th>LOS</th>\n",
       "      <th>prolonged_LOS</th>\n",
       "      <th>icu_visit</th>\n",
       "      <th>or_duration</th>\n",
       "      <th>anesth_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>467425045</td>\n",
       "      <td>134213281</td>\n",
       "      <td>225860669</td>\n",
       "      <td>1440</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>62.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GS</td>\n",
       "      <td>...</td>\n",
       "      <td>217.282759</td>\n",
       "      <td>3.846584</td>\n",
       "      <td>140.033084</td>\n",
       "      <td>0.744921</td>\n",
       "      <td>8.200501</td>\n",
       "      <td>3.493056</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>461473883</td>\n",
       "      <td>134195201</td>\n",
       "      <td>265770645</td>\n",
       "      <td>1440</td>\n",
       "      <td>35</td>\n",
       "      <td>F</td>\n",
       "      <td>50.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>OS</td>\n",
       "      <td>...</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>6.310000</td>\n",
       "      <td>4.236111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>430539801</td>\n",
       "      <td>181420324</td>\n",
       "      <td>208290342</td>\n",
       "      <td>1440</td>\n",
       "      <td>20</td>\n",
       "      <td>M</td>\n",
       "      <td>62.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>OL</td>\n",
       "      <td>...</td>\n",
       "      <td>237.222222</td>\n",
       "      <td>4.041013</td>\n",
       "      <td>139.824013</td>\n",
       "      <td>1.078906</td>\n",
       "      <td>10.044925</td>\n",
       "      <td>1.572917</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>466389608</td>\n",
       "      <td>160947402</td>\n",
       "      <td>262240911</td>\n",
       "      <td>1440</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>52.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>OL</td>\n",
       "      <td>...</td>\n",
       "      <td>217.282759</td>\n",
       "      <td>3.846584</td>\n",
       "      <td>140.033084</td>\n",
       "      <td>0.744921</td>\n",
       "      <td>8.200501</td>\n",
       "      <td>1.607639</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>439560439</td>\n",
       "      <td>163619571</td>\n",
       "      <td>279388936</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>F</td>\n",
       "      <td>65.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>OT</td>\n",
       "      <td>...</td>\n",
       "      <td>207.270270</td>\n",
       "      <td>3.867044</td>\n",
       "      <td>139.499091</td>\n",
       "      <td>0.739452</td>\n",
       "      <td>8.552474</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        op_id  subject_id    hadm_id  opdate  age sex  weight  height  asa  \\\n",
       "8   467425045   134213281  225860669    1440   60   F    62.0   154.0  1.0   \n",
       "9   461473883   134195201  265770645    1440   35   F    50.0   160.0  1.0   \n",
       "10  430539801   181420324  208290342    1440   20   M    62.0   179.0  1.0   \n",
       "11  466389608   160947402  262240911    1440   60   F    52.0   152.0  1.0   \n",
       "15  439560439   163619571  279388936       0   75   F    65.0   154.0  2.0   \n",
       "\n",
       "   department  ...    platelet potassium      sodium total_bilirubin  \\\n",
       "8          GS  ...  217.282759  3.846584  140.033084        0.744921   \n",
       "9          OS  ...  124.000000  3.900000  138.000000        0.600000   \n",
       "10         OL  ...  237.222222  4.041013  139.824013        1.078906   \n",
       "11         OL  ...  217.282759  3.846584  140.033084        0.744921   \n",
       "15         OT  ...  207.270270  3.867044  139.499091        0.739452   \n",
       "\n",
       "          wbc       LOS  prolonged_LOS  icu_visit  or_duration  \\\n",
       "8    8.200501  3.493056              1          0         70.0   \n",
       "9    6.310000  4.236111              0          0        115.0   \n",
       "10  10.044925  1.572917              1          0         90.0   \n",
       "11   8.200501  1.607639              0          0         30.0   \n",
       "15   8.552474  0.604167              0          0         15.0   \n",
       "\n",
       "    anesth_duration  \n",
       "8              90.0  \n",
       "9             150.0  \n",
       "10            135.0  \n",
       "11             90.0  \n",
       "15             25.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd32281",
   "metadata": {},
   "source": [
    "#### Preparing X and Y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d72fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 76742 entries, 8 to 128030\n",
      "Data columns (total 27 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   category_id      76742 non-null  object \n",
      " 1   age              76742 non-null  int64  \n",
      " 2   sex              76742 non-null  object \n",
      " 3   weight           76742 non-null  float64\n",
      " 4   height           76742 non-null  float64\n",
      " 5   hr               76742 non-null  float64\n",
      " 6   pip              76742 non-null  float64\n",
      " 7   pmean            76742 non-null  float64\n",
      " 8   rr               76742 non-null  float64\n",
      " 9   spo2             76742 non-null  float64\n",
      " 10  vt               76742 non-null  float64\n",
      " 11  chloride         76742 non-null  float64\n",
      " 12  creatinine       76742 non-null  float64\n",
      " 13  glucose          76742 non-null  float64\n",
      " 14  hb               76742 non-null  float64\n",
      " 15  hco3             76742 non-null  float64\n",
      " 16  lymphocyte       76742 non-null  float64\n",
      " 17  platelet         76742 non-null  float64\n",
      " 18  potassium        76742 non-null  float64\n",
      " 19  sodium           76742 non-null  float64\n",
      " 20  total_bilirubin  76742 non-null  float64\n",
      " 21  wbc              76742 non-null  float64\n",
      " 22  icu_visit        76742 non-null  int64  \n",
      " 23  or_duration      76742 non-null  float64\n",
      " 24  anesth_duration  76742 non-null  float64\n",
      " 25  department       76742 non-null  object \n",
      " 26  antype           76742 non-null  object \n",
      "dtypes: float64(21), int64(2), object(4)\n",
      "memory usage: 16.4+ MB\n"
     ]
    }
   ],
   "source": [
    "## Features to retain are those in X that will be used in training. Exludued features are features such as Operation_ID, Subject_ID..\n",
    "features_to_retain = ['category_id','age','sex',\t'weight',\t'height',\t'hr',\t'pip',\t'pmean',\t'rr',\t'spo2',\t'vt',\t'chloride',\t'creatinine',\t'glucose',\t'hb',\t'hco3',\t'lymphocyte',\t'platelet',\t'potassium',\t'sodium',\t'total_bilirubin',\t'wbc',\t'icu_visit',\t'or_duration',\t'anesth_duration',\t'department','antype'] \n",
    "\n",
    "## Create the Y, the Target - Round LOS to an integer because we dont want to predict the precison of LOS to a decimal... \n",
    "# that means they would be discharging at strange times like 2:45am (not likely)\n",
    "y = df['LOS'].round().astype(int)\n",
    "\n",
    "\n",
    "## Create X the Features for Train/Test/Validate\n",
    "X = df.drop('LOS', axis=1)\n",
    "X= X[features_to_retain]\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd613177",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc69816",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cofirm what Y looks like. (should be straing integers)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcessor Pipeline (X - features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e290fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNOTE 1 - OneHot Encoding has be REMOVED from the preproccing pipeline here because it is handled within the `catboost` model. \\n\\nNOTE 2 - If doing a single model in the pipeline, you could use the below snippet. In this case, we want to separate preprocessing pipeline so it can be used again in production (ie predicting using user-provided data)\\n\\n# Create a full pipeline by combining with an estimator, for example, a classifier\\n# pipeline_LR = Pipeline(steps=[\\n    # ('preprocessor', preprocessor),\\n    # ('model', LinearRegression())])\\n\\nTODO - If there is problem with preprocess on incomplete data - check that ctegory transformer isnt the reason\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Column Transformer with 3 types of transformers\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def cast_to_string(x):\n",
    "    return x.astype(str)\n",
    "\n",
    "COLS_TO_CAST = ['category_id','antype','sex','department', 'icu_visit']\n",
    "\n",
    "# Define the columns that you want to scale and cast to strings\n",
    "numeric_features = X.select_dtypes(include=['int', 'float']).columns\n",
    "string_features =  COLS_TO_CAST\n",
    "\n",
    "# Define transformers\n",
    "numeric_transformer = Pipeline([('impute_mean', SimpleImputer(strategy='mean')),\n",
    "                              ('scaling', StandardScaler())])\n",
    "string_transformer = FunctionTransformer(cast_to_string, validate=False)\n",
    "# categorical_transform = Pipeline([('impute_mode', SimpleImputer(strategy='most_frequent'))])\n",
    "# onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Combine transformers into a preprocessor with ColumnTransformer\n",
    "preprocessor_cat = ColumnTransformer(\n",
    "                        transformers=[\n",
    "                                    ('num', numeric_transformer, numeric_features),\n",
    "                                    \n",
    "                                    ('str', string_transformer, string_features)],\n",
    "                        remainder='passthrough')  # Leaves the rest of the columns alone)\n",
    "# ('categ', categorical_transform,string_features),\n",
    "\n",
    "'''\n",
    "NOTE 1 - OneHot Encoding has be REMOVED from the preproccing pipeline here because it is handled within the `catboost` model. \n",
    "\n",
    "NOTE 2 - If doing a single model in the pipeline, you could use the below snippet. In this case, we want to separate preprocessing pipeline so it can be used again in production (ie predicting using user-provided data)\n",
    "\n",
    "# Create a full pipeline by combining with an estimator, for example, a classifier\n",
    "# pipeline_LR = Pipeline(steps=[\n",
    "    # ('preprocessor', preprocessor),\n",
    "    # ('model', LinearRegression())])\n",
    "\n",
    "TODO - If there is problem with preprocess on incomplete data - check that ctegory transformer isnt the reason\n",
    "'''\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Package (pickle the preproccessor)\n",
    "# Preprocessor with be referenced again in production to scale/transform user-provided data.\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('../_output/preprocessor_cat.pickle', 'wb') as f:\n",
    "    pickle.dump(preprocessor_cat, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load from a file using Pickle\n",
    "try:\n",
    "    with open('../_output/preprocessor_cat.pickle', 'rb') as f:\n",
    "        preprocessor_cat = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"File 'preprocessor_catboost' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72459f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data using preprocessor pipeline.\n",
    "X = preprocessor_cat.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed5c4e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate dataset into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SPLIT = .2\n",
    "TRAINING_SPLIT = 1-TEST_SPLIT\n",
    "VALIDATION_SPLIT = .2\n",
    "\n",
    "\n",
    "def split_data(X, y, test_split=0.2, validation_split=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Splits data into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Features data.\n",
    "    - y: Target variable.\n",
    "    - test_split: Fraction of the data to be used as test set.\n",
    "    - validation_split: Fraction of the training data to be used as validation set.\n",
    "    - random_state: Seed for the random number generator.\n",
    "\n",
    "    Returns:\n",
    "    X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_split, random_state=random_state)\n",
    "\n",
    "    # Adjust validation split to account for the initial test split\n",
    "    validation_size = validation_split / (1 - test_split)\n",
    "\n",
    "    # Split the training data again into training and validation sets\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(\n",
    "        X_train, y_train, test_size=validation_size, random_state=random_state)\n",
    "\n",
    "    # Create a dictionary to hold the data splits\n",
    "    data_splits = {\n",
    "        'X_train': X_train,\n",
    "        'X_validate': X_validate,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_validate': y_validate,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "    #Print the shapes of the splits\n",
    "    for key, value in data_splits.items():\n",
    "        shape = value.shape\n",
    "        print(f\"{key} shape: {shape}\")\n",
    "        \n",
    "    return X_train, X_validate, X_test, y_train, y_validate, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (46044, 27)\n",
      "X_validate shape: (15349, 27)\n",
      "X_test shape: (15349, 27)\n",
      "y_train shape: (46044,)\n",
      "y_validate shape: (15349,)\n",
      "y_test shape: (15349,)\n"
     ]
    }
   ],
   "source": [
    "## Call data split function:\n",
    "X_train, X_validate, X_test, y_train, y_validate, y_test = split_data(X, y,test_split=TEST_SPLIT,\n",
    "    validation_split=VALIDATION_SPLIT,random_state=85100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evalution and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd80317",
   "metadata": {},
   "source": [
    "#### Create an Evaluate Function to give all metrics after model Training\n",
    "`Note`: This has been run previously and determined Catboost was most applicable for this problem. Skip to Model Hypertuning below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c247bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(true, predicted):\n",
    "    mae = mean_absolute_error(true, predicted)\n",
    "    mse = mean_squared_error(true, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(true, predicted))\n",
    "    r2_square = r2_score(true, predicted)\n",
    "    return mae, rmse, r2_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ccb8e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# `NOTE` THIS SECTION CANNOT BE RUN WITHOUT RESTORING THE 1-HOT\n",
    "# Baseline Model Training (various models)\n",
    "# \n",
    "# \n",
    "#####################\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"K-Neighbors Regressor\": KNeighborsRegressor(n_jobs=-1),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(n_jobs=-1),\n",
    "    \"XGBRegressor\": XGBRegressor(n_jobs=-1), \n",
    "    \"CatBoosting Regressor\": CatBoostRegressor(verbose=False),\n",
    "    \"AdaBoost Regressor\": AdaBoostRegressor()\n",
    "}\n",
    "model_list = []\n",
    "r2_list =[]\n",
    "\n",
    "for i in range(len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    model.fit(X_train, y_train) # Train model\n",
    "\n",
    "    # Make predictions on the VALIDATION SET\n",
    "    y_validate_pred = model.predict(X_validate)\n",
    "\n",
    "        # Evaluate on Validation dataset\n",
    "    model_validate_mae, model_validate_rmse, model_validate_r2 = evaluate_model(y_validate, y_validate_pred)\n",
    "    \n",
    "     \n",
    "    print(list(models.keys())[i])\n",
    "    model_list.append(list(models.keys())[i])\n",
    "    \n",
    "    print(f\"Model: {list(models.keys())[i]} - Validation Performance\")\n",
    "    print(f\"MAE: {model_validate_mae}, RMSE: {model_validate_rmse}, R2: {model_validate_r2}\")\n",
    "\n",
    "        \n",
    "    print('='*35)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06480b5a",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0159e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(zip(model_list, r2_list)), columns=['Model Name', 'R2_Score']).sort_values(by=[\"R2_Score\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost Parameter Tuning\n",
    "\n",
    "* Catboost does NOT require 1-hot encoding in preprocessing. \n",
    "* Instead categorical features must be declared by index.\n",
    "* PCA was not used in the problem as the large number of elements with the 'category_id' would lead to problems with overfit and/or interpretation. \n",
    "* tSNE will be considered at a later time point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_evaluate_model(true, predicted):\n",
    "    rmse = np.sqrt(mean_squared_error(true, predicted))\n",
    "    return -rmse  # Negate RMSE if you want to maximize the negative RMSE, which is common in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evaluate_model() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n    r = call_item()\n  File \"c:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"c:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n  File \"c:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n  File \"c:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"c:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n    return self.function(*args, **kwargs)\n  File \"c:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 754, in _fit_and_score\n    test_scores = _score(estimator, X_test, y_test, scorer, error_score)\n  File \"c:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n    scores = scorer(estimator, X_test, y_test)\nTypeError: evaluate_model() takes 2 positional arguments but 3 were given\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jamie\\OneDrive - Lepard & Lepard\\Data Science\\LighthouseLabs\\Python_Projects\\Prolonged_LOS_Project\\_src\\09.Catboost Model.ipynb Cell 27\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamie/OneDrive%20-%20Lepard%20%26%20Lepard/Data%20Science/LighthouseLabs/Python_Projects/Prolonged_LOS_Project/_src/09.Catboost%20Model.ipynb#X34sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m grid_search_lin \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mcatboost_model, param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, scoring\u001b[39m=\u001b[39mevaluate_model, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, error_score\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamie/OneDrive%20-%20Lepard%20%26%20Lepard/Data%20Science/LighthouseLabs/Python_Projects/Prolonged_LOS_Project/_src/09.Catboost%20Model.ipynb#X34sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Perform the grid search\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jamie/OneDrive%20-%20Lepard%20%26%20Lepard/Data%20Science/LighthouseLabs/Python_Projects/Prolonged_LOS_Project/_src/09.Catboost%20Model.ipynb#X34sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m grid_search_lin\u001b[39m.\u001b[39;49mfit(X_train, y_train, cat_features\u001b[39m=\u001b[39;49mcategorical_features_indices)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamie/OneDrive%20-%20Lepard%20%26%20Lepard/Data%20Science/LighthouseLabs/Python_Projects/Prolonged_LOS_Project/_src/09.Catboost%20Model.ipynb#X34sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Print the best hyperparameters and corresponding MSE score\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jamie/OneDrive%20-%20Lepard%20%26%20Lepard/Data%20Science/LighthouseLabs/Python_Projects/Prolonged_LOS_Project/_src/09.Catboost%20Model.ipynb#X34sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest hyperparameters found:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\concurrent\\futures\\_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    457\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    459\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\jamie\\anaconda3\\envs\\Lighthouse_env\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: evaluate_model() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "# Declare the categorical features that would be 1-hot encoded (not all the categorical cols)\n",
    "# This only needs to be done during training model (ie not required as part of preprocessing)\n",
    "categorical_features_indices =[0,2,22,25,26]\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'iterations': [100, 200, 300],      # Number of boosting iterations\n",
    "    'depth': [6, 8, 10],                # Depth of trees\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate  \n",
    "    }\n",
    "\n",
    "# Create a CatBoostRegressor model\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search_lin = GridSearchCV(estimator=catboost_model, param_grid=param_grid, cv=5, scoring=grid_evaluate_model, n_jobs=-1, error_score='raise')\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search_lin.fit(X_train, y_train, cat_features=categorical_features_indices)\n",
    "\n",
    "# Print the best hyperparameters and corresponding MSE score\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(grid_search_lin.best_params_)\n",
    "print(\"Best RMSE score:\", -grid_search_lin.best_score_)\n",
    "\n",
    "# Get the best trained model\n",
    "best_catboost_model_lin = grid_search_lin.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "validation_predictions = best_catboost_model_lin.predict(X_validate)\n",
    "\n",
    "########################\n",
    "## Output:\n",
    "########################\n",
    "'''Best hyperparameters found:\n",
    "{'depth': 10, 'iterations': 300, 'learning_rate': 0.1}\n",
    "Best RMSE score: 3.834154628241867'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "#  OPTIMIZED - CatBoost\n",
    "# \n",
    "########################\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import Pool\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "###############\n",
    "# Specify categorical feature indices\n",
    "categorical_features_indices = [0, 2, 22,25, 26]\n",
    "\n",
    "# Create the Pool for training data\n",
    "train_pool = Pool(data=X_train, label=y_train, cat_features=categorical_features_indices)\n",
    "\n",
    "# If you have a validation dataset\n",
    "validation_pool = Pool(data=X_validate, label=y_validate, cat_features=categorical_features_indices)\n",
    "\n",
    "#########\n",
    "# Best hyperparameters found (after hyperparameter tuning)\n",
    "best_params = {'depth': 8, 'iterations': 200, 'learning_rate': 0.1}\n",
    "\n",
    "# Instantiate CatBoostRegressor with the best hyperparameters\n",
    "cat_model_lin = CatBoostRegressor(**best_params)\n",
    "\n",
    "cat_model_lin.fit(\n",
    "    train_pool,\n",
    "    eval_set=validation_pool,  # Remove this if you don't have a validation set\n",
    "    verbose=10,  # This will print the progress every 10 iterations\n",
    "    plot=True    # This will plot the learning curve (only works in Jupyter notebooks)\n",
    ")\n",
    "\n",
    "# because of the Pool, the data here should not be the Pool object but raw data.\n",
    "y_pred_cat_lin_r2 = cat_model_lin.predict(X_test)  \n",
    "\n",
    "# Evaluate on Test dataset\n",
    "model_test_mae, model_test_rmse, model_test_r2 = evaluate_model(y_test, y_pred_cat_lin_r2)\n",
    "\n",
    "\n",
    "\n",
    "########################\n",
    "## Output:\n",
    "########################\n",
    "'''bestTest = 3.863708828\n",
    "bestIteration = 177\n",
    "\n",
    "Shrink model to first 178 iterations.\n",
    "R-squared of base model: 0.5210853587572815\n",
    "RMSE of the base model: 3.864'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
